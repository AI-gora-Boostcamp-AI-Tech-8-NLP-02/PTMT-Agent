import json
import re
from typing import Any, Dict, List, Tuple
from dotenv import load_dotenv

from core.prompts.concept_expansion.v4 import CONCEPT_EXPANSION_PROMPT_V4
from langchain.agents import create_agent
from langchain_tavily import TavilySearch

from core.contracts.concept_expansion import ConceptExpansionInput, ConceptExpansionOutput
from core.contracts.types.curriculum import CurriculumGraph, KeywordNode
from core.utils.get_message import get_last_ai_message
from core.utils.timeout import async_timeout

load_dotenv()

KEYWORD_ID_PATTERN = re.compile(r"^key-(\d{3})$")

class ConceptExpansionAgent:
    def __init__(self, llm):
        self.llm = llm
        
        self.tools = [
            TavilySearch(max_results=3, topic="general")
        ]
        
        self.agent = create_agent(
            model=self.llm,
            tools=self.tools
        )
    
    @async_timeout(45)
    async def run(self, input: ConceptExpansionInput) -> ConceptExpansionOutput:
        # Input ì¶”ì¶œ
        keyword_graph = self._extract_keyword_graph(input["curriculum"])
        paper_info = input["curriculum"]["graph_meta"]
        
        # í”„ë¡¬í”„íŠ¸ ì ìš©
        messages = CONCEPT_EXPANSION_PROMPT_V4.format_messages(
            paper_info = paper_info,
            keyword_graph=json.dumps(
                keyword_graph, ensure_ascii=False
            ),
            reason=input["keyword_expand_reason"],
            keyword_ids = input["missing_concepts"],
            user_level = input["user_info"]["level"],
            known_concept=input["user_info"]["known_concept"]
        )

        # agent ì‹¤í–‰
        response = await self.agent.ainvoke(
            {
                "messages": messages
            },
            config={
                "max_tokens": 1024,
                "tags": [
                    "agent:concept-expansion",
                    "prompt:v2",
                    "tool:tavily",
                ],
            }
        )
        
        # llm ê²°ê³¼ parsing
        expanded_graph = self._parse_response(response)

        expanded_graph = self._filter_known_concepts(
            expanded_graph=expanded_graph,
            known_concepts=input["user_info"]["known_concept"]
        )
        
        # normalization
        expanded_graph = self._normalize_expanded_graph(
            paper_id=input["curriculum"]["graph_meta"]["paper_id"],
            base_graph=keyword_graph,
            expanded_graph=expanded_graph,
        )
        
        # ê¸°ì¡´ ê·¸ë˜í”„ì™€ ë³‘í•©
        expanded_graph = self._merge_graph(keyword_graph, expanded_graph)
        
        # ê¸°ì¡´ ì»¤ë¦¬í˜ëŸ¼ê³¼ ë³‘í•©
        merged_curriculum = self._merge_expansion_into_curriculum(input["curriculum"], expanded_graph)
        
        result: ConceptExpansionOutput = {
            "curriculum": merged_curriculum
        }

        return result

    def _parse_response(self, response) -> Dict[str, Any]:
        ai_message = get_last_ai_message(response)
        
        try:
            parsed = json.loads(ai_message.content)
            expanded_graph = parsed.get("expanded_graph")
        except Exception:
            return {"nodes": [], "edges": []}

        if not self._is_valid_expanded_graph(expanded_graph):
            return {"nodes": [], "edges": []}
        
        return expanded_graph

    def _normalize_expanded_graph(
        self,
        paper_id: str,
        base_graph: Dict[str, Any],
        expanded_graph: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        - keyword_id ì¬ë¶€ì—¬
        - edge ì¬ë§¤í•‘
        """
        if not expanded_graph.get("nodes"):
            return {"nodes": [], "edges": []}

        return self._reassign_keyword_ids(
            paper_id=paper_id,
            base_graph=base_graph,
            expanded_graph=expanded_graph,
        )
    
    def _extract_keyword_graph(self, curriculum_graph: CurriculumGraph) -> Dict[str, Any]:
        """
        Curriculum ê·¸ë˜í”„ì—ì„œ keyword ë…¸ë“œë§Œ ì¶”ì¶œí•˜ì—¬
        í‘œì¤€í™”ëœ keyword ê·¸ë˜í”„ êµ¬ì¡°ë¡œ ë³€í™˜í•œë‹¤.
        """

        nodes = curriculum_graph.get("nodes", [])
        edges = curriculum_graph.get("edges", [])

        # 1. keyword ë…¸ë“œë§Œ ì¶”ì¶œ
        keyword_nodes = [
            {
                "keyword_id": node["keyword_id"],
                "keyword": node.get("keyword"),
                "description": node.get("description", "")
            }
            for node in nodes
        ]

        return {
            "nodes": keyword_nodes,
            "edges": edges
        }

    def _reassign_keyword_ids(
        self,
        paper_id: str,
        base_graph: Dict[str, Any],
        expanded_graph: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        base_graphì˜ keyword_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ
        expanded_graphì˜ nodeë“¤ì— ìƒˆë¡œìš´ keyword_idë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë¶€ì—¬í•œë‹¤.
        """
        
        id_mapping: Dict[str, str] = {}

        # base_graphì—ì„œ ë§ˆì§€ë§‰ keyword ë²ˆí˜¸ ì°¾ê¸°
        max_index = -1
        for node in base_graph.get("nodes", []):
            match = KEYWORD_ID_PATTERN.match(node.get("keyword_id", ""))
            if match:
                node_id = node.get("keyword_id")
                id_mapping[node_id] = node_id
                max_index = max(max_index, int(match.group(1)))

        next_index = max_index + 1

        # expanded_graph nodeì— ìƒˆ keyword_id ë¶€ì—¬
        new_nodes: List[Dict[str, Any]] = []

        for node in expanded_graph.get("nodes", []):
            old_id = node.get("keyword_id")

            new_id = f"key-{next_index:03d}"
            next_index += 1

            id_mapping[old_id] = new_id

            new_node = dict(node)
            new_node["keyword_id"] = new_id
            new_node.setdefault("description", None)

            new_nodes.append(new_node)

        # edge ì¬ë§¤í•‘
        new_edges = []
        for edge in expanded_graph.get("edges", []):
            start = edge.get("start")
            end = edge.get("end")
            
            if end == paper_id and start in id_mapping:
                new_edges.append(
                    {
                        "start": id_mapping[start],
                        "end": end,
                    }
                )
            elif start in id_mapping and end in id_mapping:
                new_edges.append(
                    {
                        "start": id_mapping[start],
                        "end": id_mapping[end],
                    }
                )

        return {
            "nodes": new_nodes,
            "edges": new_edges,
        }

    def _merge_graph(
        self,
        base_graph: Dict[str, Any],
        expanded_graph: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ê¸°ì¡´ curriculum graphì™€ expanded_graphë¥¼ ë³‘í•©í•œë‹¤.
        - keyword_id ê¸°ì¤€ìœ¼ë¡œ node ì¤‘ë³µ ì œê±°
        - (start, end) ê¸°ì¤€ìœ¼ë¡œ edge ì¤‘ë³µ ì œê±°
        - ê¸°ì¡´ êµ¬ì¡°ëŠ” ìœ ì§€í•˜ê³  ì¶”ê°€ë§Œ ìˆ˜í–‰
        """

        # ---- Nodes ë³‘í•© ----
        base_nodes = base_graph.get("nodes", [])
        expanded_nodes = expanded_graph.get("nodes", [])

        node_map = {
            node["keyword_id"]: node
            for node in base_nodes
        }

        for node in expanded_nodes:
            keyword_id = node["keyword_id"]

            # description ì—†ìœ¼ë©´ Noneìœ¼ë¡œ ë³´ì •
            if "description" not in node:
                node["description"] = None
            else: 
                node["description"] = None

            if keyword_id not in node_map:
                node_map[keyword_id] = node

        merged_nodes = list(node_map.values())

        # ---- Edges ë³‘í•© ----
        base_edges = base_graph.get("edges", [])
        expanded_edges = expanded_graph.get("edges", [])

        edge_set: set[Tuple[str, str]] = {
            (edge["start"], edge["end"])
            for edge in base_edges
        }

        for edge in expanded_edges:
            edge_set.add((edge["start"], edge["end"]))

        merged_edges = [
            {"start": start, "end": end}
            for start, end in edge_set
        ]

        return {
            "nodes": merged_nodes,
            "edges": merged_edges
        }

    def _is_valid_expanded_graph(self, expanded_graph: Dict[str, Any]) -> bool:
        """
        expanded_graphê°€ ê¸°ëŒ€í•œ êµ¬ì¡°ë¥¼ ë§Œì¡±í•˜ëŠ”ì§€ ê²€ì¦í•œë‹¤.
        """

        if not isinstance(expanded_graph, dict):
            return False

        nodes = expanded_graph.get("nodes")
        edges = expanded_graph.get("edges")

        if not isinstance(nodes, list) or not isinstance(edges, list):
            return False

        # ---- node ê²€ì¦ ----
        for node in nodes:
            if not isinstance(node, dict):
                return False
            if "keyword_id" not in node or "keyword" not in node:
                return False
            if not isinstance(node["keyword_id"], str):
                return False
            if not isinstance(node["keyword"], str):
                return False

        # ---- edge ê²€ì¦ ----
        for edge in edges:
            if not isinstance(edge, dict):
                return False
            if "start" not in edge or "end" not in edge:
                return False
            if not isinstance(edge["start"], str):
                return False
            if not isinstance(edge["end"], str):
                return False
            # reasonì€ optionalì´ì§€ë§Œ, ìˆìœ¼ë©´ stringì´ì–´ì•¼ í•¨
            if "reason" in edge and not isinstance(edge["reason"], str):
                return False

        return True

    def _merge_expansion_into_curriculum(
        self,
        curriculum: CurriculumGraph,
        expansion_result: Dict[str, Any]
    ) -> CurriculumGraph:
        """
        CurriculumGraph ì—…ë°ì´íŠ¸ í•œë‹¤.
        """

        expanded_nodes = expansion_result.get("nodes", [])
        expanded_edges = expansion_result.get("edges", [])

        # ---- ê¸°ì¡´ ë…¸ë“œ ë§µ ----
        node_map: Dict[str, KeywordNode] = {
            node["keyword_id"]: node
            for node in curriculum["nodes"]
        }

        # ---- ìƒˆ ë…¸ë“œë§Œ ì¶”ê°€ ----
        for raw_node in expanded_nodes:
            kid = raw_node.get("keyword_id")
            if not kid:
                continue

            if kid not in node_map:
                node_map[kid] = {
                    "keyword_id": kid,
                    "keyword": raw_node.get("keyword", ""),
                    "description": "",
                    "keyword_importance": None,
                    "is_keyword_necessary": None,
                    "is_resource_sufficient": False,
                    "resources": []
                }

        merged_nodes = list(node_map.values())

        # ---- Edge ë³‘í•© ----
        edge_set: set[Tuple[str, str]] = {
            (edge["start"], edge["end"])
            for edge in curriculum["edges"]
        }

        for edge in expanded_edges:
            start, end = edge.get("start"), edge.get("end")
            if not start or not end:
                continue
            edge_set.add((start, end))

        merged_edges = [
            {"start": start, "end": end}
            for start, end in edge_set
        ]

        return {
            "graph_meta": curriculum["graph_meta"],
            "nodes": merged_nodes,
            "edges": merged_edges
        }


    def _filter_known_concepts(
        self, 
        expanded_graph: Dict[str, Any], 
        known_concepts: List[str]
    ) -> Dict[str, Any]:
        """
        LLMì´ ìƒì„±í•œ í™•ì¥ ê·¸ë˜í”„ì—ì„œ
        known_conceptê³¼ ì¼ì¹˜í•˜ëŠ” ë…¸ë“œ ì œê±°
        """
        
        # ì†Œë¬¸ì ë³€í™˜ ë° ê³µë°± ì œê±° 
        def normalize(text):
            return text.strip().lower().replace(" ", "") if text else ""

        known_set = {normalize(k) for k in known_concepts}
        
        dropped_ids = set()
        valid_nodes = []

        # Node í•„í„°ë§
        for node in expanded_graph.get("nodes", []):
            raw_keyword = node.get("keyword", "")
            norm_keyword = normalize(raw_keyword)
            node_id = node.get("keyword_id")

            # í‚¤ì›Œë“œê°€ known_setì— ìˆìœ¼ë©´ ì œê±°
            if norm_keyword in known_set:
                # print(f"ğŸš« Known Concept í•„í„°ë§ë¨: {raw_keyword}")
                dropped_ids.add(node_id)
                continue
            
            valid_nodes.append(node)

        # Edge í•„í„°ë§
        valid_edges = []
        for edge in expanded_graph.get("edges", []):
            start = edge.get("start")
            end = edge.get("end")

            # startë‚˜ end ì¤‘ í•˜ë‚˜ë¼ë„ ì œê±°ëœ IDë¼ë©´ ì´ ì—£ì§€ëŠ” ë²„ë¦¼
            if start in dropped_ids or end in dropped_ids:
                continue
            
            valid_edges.append(edge)

        return {
            "nodes": valid_nodes,
            "edges": valid_edges
        }