import asyncio
import json
import os
from dotenv import load_dotenv
from typing import Literal

from langgraph.graph import StateGraph, START, END

from core.graphs.subgraph_to_curriculum import transform_subgraph_to_final_curriculum
from core.graphs.series.nodes import (
    curriculum_orchestrator_node, 
    resource_discovery_agent_node,
    curriculum_compose_node,
    concept_expansion_node,
    paper_concept_alignment_node
)
from core.graphs.series.state_definition import CreateCurriculumOverallState


def agent_loop_router(state: CreateCurriculumOverallState) -> Literal["resource_discovery", "concept_expansion", "paper_concept_alignment", "orchestrator"]:
    tasks = state.get("tasks", [])

    # tasksê°€ ë‚¨ì•„ìˆìœ¼ë©´ ë‹¤ìŒ agentë¡œ ì´ë™
    if tasks:
        next_task = tasks[0] # í˜¹ì€ ìš°ì„ ìˆœìœ„ ë¡œì§
        print(f"ğŸ”„ [Agent] ({next_task}) -> ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ ì´ë™")
        if next_task == "generate_description": return "paper_concept_alignment"    
        if next_task == "resource_search": return "resource_discovery"
        if next_task == "keyword_expansion": return "concept_expansion"

            
    print("âœ… [Agent] í•  ì¼ ëª©ë¡ ë¹„ì–´ìˆìŒ -> Orchestratorë¡œ ë³µê·€í•˜ì—¬ ì¬ì§„ë‹¨")
    return "orchestrator"

def orchestrator_router(state: CreateCurriculumOverallState) -> Literal["resource_discovery", "concept_expansion", "paper_concept_alignment", "curriculum_compose"]:
    tasks = state.get("tasks", [])
    
    current_count = state.get("current_iteration_count", 0)
    MAX_ITERATIONS = 6


    if not tasks: 
        return "curriculum_compose"
        
    next_task = tasks[0] 

    # ì¢…ë£Œ í™•ì¸
    if next_task == "curriculum_compose":
        print("ğŸ ìµœì¢… ë‹¨ê³„(Compose)ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return "curriculum_compose"

    if current_count >= MAX_ITERATIONS:
        print("âš ï¸ ë°˜ë³µ íšŸìˆ˜ ì´ˆê³¼. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return "curriculum_compose" 

    # agnet ë°°ì •
    print(f"ğŸ”„ [Agent] ({next_task}) -> ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ ì´ë™") 
    if next_task == "generate_description": return "paper_concept_alignment"
    if next_task == "resource_search": return "resource_discovery"
    if next_task == "keyword_expansion": return "concept_expansion"

    return "curriculum_compose"



# ì „ì²˜ë¦¬ í•¨ìˆ˜
def format_paper_content(paper_json: dict) -> str:
    title = paper_json.get("title", "Unknown Title")
    abstract = paper_json.get("abstract", "")
    body = paper_json.get("body", [])
    text = f"# {title}\n\n## Abstract\n{abstract}\n\n"
    for section in body:
        text += f"### {section.get('subtitle', '')}\n{section.get('text', '')}\n\n"
    return text

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
async def run_langgraph_workflow():
    load_dotenv()
    
    # dummy ë°ì´í„° ê²½ë¡œ ì„¤ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    user_info_path = os.path.join(current_dir, "../../../dummy_data/dummy_user_information.json")
    paper_content_path = os.path.join(current_dir, "../../../dummy_data/dummy_parsing_paper_v2.json")
    subgraph_path = os.path.join(current_dir, "../../../dummy_data/dummy_subgraph.json")
    
    meta_data_input = {
        "paper_id": "123456",
        "title" : "Attention Is All You Need",
        "summarize": "ì´ ë…¼ë¬¸ì€ ê¸°ì¡´ì˜ RNNì´ë‚˜ CNNì„ ì™„ì „íˆ ë°°ì œí•˜ê³  ì˜¤ë¡œì§€ ì–´í…ì…˜(Attention) ë©”ì»¤ë‹ˆì¦˜ë§Œìœ¼ë¡œ êµ¬ì„±ëœ íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ì•„í‚¤í…ì²˜ë¥¼ ì œì‹œí•˜ë©° ë”¥ëŸ¬ë‹ ì—°êµ¬ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì—´ì—ˆìŠµë‹ˆë‹¤. ì—°ì‚°ì˜ ë³‘ë ¬í™”ë¥¼ í†µí•´ í•™ìŠµ ì†ë„ë¥¼ ë¹„ì•½ì ìœ¼ë¡œ ë†’ì˜€ì„ ë¿ë§Œ ì•„ë‹ˆë¼, ê¸°ì¡´ ëª¨ë¸ë“¤ì˜ ê³ ì§ˆì ì¸ ë¬¸ì œì˜€ë˜ ì¥ê±°ë¦¬ ì˜ì¡´ì„± ë¬¸ì œë¥¼ í•´ê²°í•¨ìœ¼ë¡œì¨ í˜„ì¬ GPTì™€ ê°™ì€ ì´ˆê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ë“¤ì´ íƒ„ìƒí•  ìˆ˜ ìˆëŠ” ê²°ì •ì ì¸ í† ëŒ€ë¥¼ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤."
    }
    
    # ë°ì´í„° ë¡œë“œ
    try:
        with open(subgraph_path, "r", encoding="utf-8") as f:
            dummy_subgraph = json.load(f)
        with open(user_info_path, "r", encoding="utf-8") as f:
            user_info_data = json.load(f)
        with open(paper_content_path, "r", encoding="utf-8") as f:
            paper_raw_data = json.load(f)
    except FileNotFoundError as e:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return

    curriculum_data = transform_subgraph_to_final_curriculum(dummy_subgraph, meta_data_input)

    # ì´ˆê¸° State êµ¬ì„± 
    initial_state = {
        "paper_name": paper_raw_data.get("title", "Unknown"),
        "paper_summary": paper_raw_data.get("abstract", ""),
        "initial_keywords": [n.get("keyword") for n in curriculum_data.get("nodes", [])],
        "paper_content": paper_raw_data,
        "user_info": user_info_data,
        "curriculum": curriculum_data,
        "tasks": [],
        "current_iteration_count": 0, # ì‹œì‘ ì¹´ìš´íŠ¸ 0
        "is_keyword_sufficient": False,
        "is_resource_sufficient": False,
        "needs_description_ids": [],
        "insufficient_resource_ids": [],
        "missing_concepts": [],
        "keyword_reasoning": "Init",
        "resource_reasoning": "Init",
        "keyword_expand_reason": ""
    }



    # StateGraph êµ¬ì„±
    workflow = StateGraph(CreateCurriculumOverallState)

    # ë…¸ë“œ ë“±ë¡
    workflow.add_node("orchestrator", curriculum_orchestrator_node)
    workflow.add_node("resource_discovery", resource_discovery_agent_node)
    workflow.add_node("curriculum_compose", curriculum_compose_node)
    workflow.add_node("concept_expansion", concept_expansion_node)
    workflow.add_node("paper_concept_alignment", paper_concept_alignment_node)

    # ì—£ì§€ ì—°ê²°
    workflow.add_edge(START, "orchestrator")
    
    # orchestrator edge
    workflow.add_conditional_edges(
        "orchestrator",
        orchestrator_router,
        {
            "resource_discovery": "resource_discovery",
            "paper_concept_alignment": "paper_concept_alignment",
            "concept_expansion": "concept_expansion",
            "curriculum_compose": "curriculum_compose"
        }
    )

    # agent edge
    workflow.add_conditional_edges(
        "resource_discovery", 
        agent_loop_router, 
        {
            "resource_discovery": "resource_discovery",
            "concept_expansion": "concept_expansion",
            "paper_concept_alignment": "paper_concept_alignment",
            "orchestrator": "orchestrator" 
        }
    )

    workflow.add_conditional_edges(
        "paper_concept_alignment", 
        agent_loop_router, 
        {
            "resource_discovery": "resource_discovery",
            "concept_expansion": "concept_expansion",
            "paper_concept_alignment": "paper_concept_alignment",
            "orchestrator": "orchestrator" 
        }
    )

    workflow.add_conditional_edges(
        "concept_expansion", 
        agent_loop_router, 
        {
            "resource_discovery": "resource_discovery",
            "concept_expansion": "concept_expansion",
            "paper_concept_alignment": "paper_concept_alignment",
            "orchestrator": "orchestrator" 
        }
    )

    # ìµœì¢… edge
    workflow.add_edge("curriculum_compose",END)

    # ì»´íŒŒì¼
    app = workflow.compile()

    # ì‹¤í–‰
    print("\nğŸš€ LangGraph ì›Œí¬í”Œë¡œìš° ê°€ë™...")
    final_state = await app.ainvoke(initial_state)

    # ê²°ê³¼ ë¶„ì„
    print("\n" + "="*50)
    print("ğŸ¯ ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸")
    print("="*50)
    print(f"ğŸ“Š ìµœì¢… ë°˜ë³µ íšŸìˆ˜: {final_state.get('current_iteration_count')}")
    print(f"ğŸ“Š ë‚¨ì€ Tasks: {final_state.get('tasks')}")
    print(f"âœ… ì „ì²´ ë¦¬ì†ŒìŠ¤ ì¶©ë¶„ì„±: {final_state.get('is_resource_sufficient')}")
    
    # ê°œë³„ ë…¸ë“œ ìƒíƒœ í™•ì¸
    for node in final_state['curriculum']['nodes']:
        print(f"  - [{node['keyword_id']}] {node['keyword']}: Sufficient={node['is_resource_sufficient']}, Resources={len(node.get('resources', []))}")
    
    with open("langgraph_test_result_series_final.json", "w", encoding="utf-8") as f:
        json.dump(final_state, f, indent=2, ensure_ascii=False)
    print("\nâœ… 'langgraph_test_result_full.json' ì €ì¥ ì™„ë£Œ.")

if __name__ == "__main__":
    asyncio.run(run_langgraph_workflow())