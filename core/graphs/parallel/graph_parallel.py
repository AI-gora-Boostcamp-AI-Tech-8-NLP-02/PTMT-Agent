import asyncio
import json
import os
from dotenv import load_dotenv
from typing import Literal, List, Dict, Any

from langgraph.graph import StateGraph, START, END


from core.graphs.subgraph_to_curriculum import transform_subgraph_to_final_curriculum
from core.graphs.parallel.nodes_parallel import (
    curriculum_orchestrator_node, 
    resource_discovery_agent_node,
    curriculum_compose_node,
    concept_expansion_node,
    paper_concept_alignment_node,
    first_node_order_node
)
from core.graphs.parallel.state_parallel import CreateCurriculumOverallState

def create_initial_state(
    subgraph_data: Dict[str, Any],
    user_info_data: Dict[str, Any],
    paper_raw_data: Dict[str, Any],
    paper_meta_data: Dict[str,Any],
    initial_keywords: List[str]
) -> CreateCurriculumOverallState:
    """
    ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„ Transformì„ ìˆ˜í–‰
    LangGraph ì‹¤í–‰ì„ ìœ„í•œ Initial Stateë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜
    """
    # Subgraph -> Curriculum ë³€í™˜ (Transform)
    curriculum_data = transform_subgraph_to_final_curriculum(subgraph_data, paper_meta_data)

    # Initial State 
    initial_state = {
        "paper_name": paper_raw_data.get("title", "Unknown"),
        "paper_summary": paper_meta_data.get("summarize", ""),
        "initial_keywords": initial_keywords,
        "paper_content": paper_raw_data,
        "user_info": user_info_data,
        "curriculum": curriculum_data,
        "tasks": [],
        "current_iteration_count": 0, # ì‹œì‘ ì¹´ìš´íŠ¸ 0
        "is_keyword_sufficient": False,
        "is_resource_sufficient": False,
        "needs_description_ids": [],
        "insufficient_resource_ids": [],
        "missing_concepts": [],
        "keyword_reasoning": "Init",
        "resource_reasoning": "Init",
        "keyword_expand_reason": ""
    }

    return CreateCurriculumOverallState(**initial_state)

# Router: ë³‘ë ¬ ì‹¤í–‰
def orchestrator_router(state: CreateCurriculumOverallState) -> List[str]:
    tasks = state.get("tasks", [])
    current_count = state.get("current_iteration_count", 0)
    MAX_ITERATIONS = 6

    # ë³‘ë ¬ ì‹¤í–‰í•  ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ 
    next_nodes = []
    is_over_limit = current_count >= MAX_ITERATIONS

    has_desc = "generate_description" in tasks
    has_res = "resource_search" in tasks
    has_exp = "keyword_expansion" in tasks

    is_critical_cleanup = has_desc and has_res

    if is_over_limit:
        # ë‘˜ ë‹¤ ë™ì‹œì— ìˆì„ ë•Œë§Œ ì‹¤í–‰
        if is_critical_cleanup:
            print(f"âš ï¸ [Router] ë°˜ë³µ ì´ˆê³¼({current_count})! ê·¸ëŸ¬ë‚˜ 'ì„¤ëª…,ìë£Œ'ê°€ ë™ì‹œì— ëˆ„ë½ë˜ì–´ ë§ˆì§€ë§‰ìœ¼ë¡œ ë³´ì¶©í•©ë‹ˆë‹¤.")
            next_nodes.append("paper_concept_alignment")
            next_nodes.append("resource_discovery")
        else:
            print(f"ğŸ›‘ [Router] ë°˜ë³µ ì´ˆê³¼. (ì„¤ëª…,ìë£Œ ë™ì‹œ ëˆ„ë½ ì¡°ê±´ ë¶ˆë§Œì¡±) -> ê°•ì œ ì¢…ë£Œ.")
            return ["curriculum_compose"]
    else:
        # ì œí•œ ì•ˆ ë„˜ì—ˆìœ¼ë©´ ìˆëŠ” íƒœìŠ¤í¬ ë‹¤ ë‹´ê¸°
        if has_desc: next_nodes.append("paper_concept_alignment")
        if has_res: next_nodes.append("resource_discovery")
        if has_exp: next_nodes.append("concept_expansion")

    if next_nodes:
        print(f"ğŸ”€ [Parallel] ë™ì‹œ ì‹¤í–‰: {next_nodes} (Loop: {current_count})")
        return next_nodes

    print("âœ… [Router] ì‹¤í–‰ ê°€ëŠ¥í•œ íƒœìŠ¤í¬ ì—†ìŒ. ì¢…ë£Œ.")
    return ["curriculum_compose"]


# Join Node, ê²°ê³¼ ë³‘í•© í›„ì²˜ë¦¬
async def join_parallel_results_node(state: CreateCurriculumOverallState):
    """
    ë³‘ë ¬ ì‹¤í–‰ëœ ë…¸ë“œë“¤ì´ ë¦¬ë“€ì„œë¥¼ í†µí•´ ë°ì´í„°ë¥¼ í•©ì¹˜ê³  task ì •ë¦¬
    """
    print(" [Join] ë³‘ë ¬ ì‘ì—… ì™„ë£Œ. Task ëª©ë¡ ì •ë¦¬ ì¤‘...")
    remaining_tasks = [ ]
    
    return {
        "tasks": remaining_tasks
    }


def run_langgraph_workflow():
    # StateGraph êµ¬ì„±
    workflow = StateGraph(CreateCurriculumOverallState)

    # ë…¸ë“œ ë“±ë¡
    workflow.add_node("orchestrator", curriculum_orchestrator_node)
    workflow.add_node("resource_discovery", resource_discovery_agent_node)
    workflow.add_node("paper_concept_alignment", paper_concept_alignment_node)
    workflow.add_node("concept_expansion", concept_expansion_node)
    workflow.add_node("curriculum_compose", curriculum_compose_node)
    workflow.add_node("first_node_order",first_node_order_node)
    
    # join ë…¸ë“œ ë“±ë¡
    workflow.add_node("join_results", join_parallel_results_node)

    # ì—£ì§€ ì—°ê²°
    workflow.add_edge(START, "orchestrator")
    
    # ë³‘ë ¬ ë…¸ë“œë“¤
    # ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ map ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ì™€ ì¼ì¹˜í•˜ëŠ” ë…¸ë“œë“¤ì´ ë™ì‹œ ì‹¤í–‰ë¨
    workflow.add_conditional_edges(
        "orchestrator",
        orchestrator_router,
        {
            "resource_discovery": "resource_discovery",
            "paper_concept_alignment": "paper_concept_alignment",
            "concept_expansion": "concept_expansion",
            "curriculum_compose": "curriculum_compose"
        }
    )

    
    # ì¼ì´ ëë‚˜ë©´ ë¬´ì¡°ê±´ Join ë…¸ë“œë¡œ ëª¨ì„
    workflow.add_edge("resource_discovery", "join_results")
    workflow.add_edge("paper_concept_alignment", "join_results")
    workflow.add_edge("concept_expansion", "join_results")
    workflow.add_edge("join_results", "orchestrator")

    # ì¢…ë£Œ ì²˜ë¦¬
    workflow.add_edge("curriculum_compose", "first_node_order")
    workflow.add_edge("first_node_order",END)

    # ì»´íŒŒì¼
    return workflow.compile()