import asyncio
import json
import os
from dotenv import load_dotenv
from typing import Literal, List

from langgraph.graph import StateGraph, START, END


from core.graphs.subgraph_to_curriculum import transform_subgraph_to_final_curriculum
from core.graphs.parallel.nodes_parallel import (
    curriculum_orchestrator_node, 
    resource_discovery_agent_node,
    curriculum_compose_node,
    concept_expansion_node,
    paper_concept_alignment_node
)
from core.graphs.parallel.state_parallel import CreateCurriculumOverallState


# Router: ë³‘ë ¬ ì‹¤í–‰
def orchestrator_router(state: CreateCurriculumOverallState) -> List[str]:
    tasks = state.get("tasks", [])
    current_count = state.get("current_iteration_count", 0)
    MAX_ITERATIONS = 6

    # ì¢…ë£Œ ì¡°ê±´: íƒœìŠ¤í¬ê°€ ì—†ê±°ë‚˜ ë°˜ë³µ íšŸìˆ˜ ì´ˆê³¼ ì‹œ
    if not tasks: 
        print("ğŸ [Router] ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ. ìµœì¢… ë‹¨ê³„ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        return ["curriculum_compose"]
    
    if current_count >= MAX_ITERATIONS:
        print("âš ï¸ [Router] ë°˜ë³µ íšŸìˆ˜ ì´ˆê³¼. ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return ["curriculum_compose"]

    # 2. ë³‘ë ¬ ì‹¤í–‰í•  ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ 
    next_nodes = []
    
    # tasks ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” í‚¤ì›Œë“œë¥¼ ë³´ê³  ì‹¤í–‰í•  ë…¸ë“œë¥¼ ê²°ì •
    if "generate_description" in tasks: 
        next_nodes.append("paper_concept_alignment")
    if "resource_search" in tasks: 
        next_nodes.append("resource_discovery")
    if "keyword_expansion" in tasks: 
        next_nodes.append("concept_expansion")

    # tasksì—ëŠ” ìˆëŠ”ë° ë§¤í•‘ëœ ë…¸ë“œê°€ ì—†ëŠ” ê²½ìš°
    if not next_nodes:
        return ["curriculum_compose"]
        
    print(f"ğŸ”€ [Parallel] ë‹¤ìŒ ì—ì´ì „íŠ¸ë“¤ ë™ì‹œ ì‹¤í–‰: {next_nodes} (Loop: {current_count})")
    
    return next_nodes


# Join Node, ê²°ê³¼ ë³‘í•© í›„ì²˜ë¦¬
async def join_parallel_results_node(state: CreateCurriculumOverallState):
    """
    ë³‘ë ¬ ì‹¤í–‰ëœ ë…¸ë“œë“¤ì´ ë¦¬ë“€ì„œë¥¼ í†µí•´ ë°ì´í„°ë¥¼ í•©ì¹˜ê³  task ì •ë¦¬
    """
    print(" [Join] ë³‘ë ¬ ì‘ì—… ì™„ë£Œ. Task ëª©ë¡ ì •ë¦¬ ì¤‘...")
    remaining_tasks = [ ]
    
    return {
        "tasks": remaining_tasks
    }


async def run_langgraph_workflow():
    load_dotenv()
    
    # dummy ë°ì´í„° ê²½ë¡œ ì„¤ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    user_info_path = os.path.join(current_dir, "../../../dummy_data/dummy_user_information.json")
    paper_content_path = os.path.join(current_dir, "../../../dummy_data/dummy_parsing_paper_v2.json")
    subgraph_path = os.path.join(current_dir, "../../../dummy_data/dummy_subgraph.json")
    
    meta_data_input = {
        "paper_id": "123456",
        "title" : "Attention Is All You Need",
        "summarize": "ì´ ë…¼ë¬¸ì€ ê¸°ì¡´ì˜ RNNì´ë‚˜ CNNì„ ì™„ì „íˆ ë°°ì œí•˜ê³  ì˜¤ë¡œì§€ ì–´í…ì…˜(Attention) ë©”ì»¤ë‹ˆì¦˜ë§Œìœ¼ë¡œ êµ¬ì„±ëœ íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ì•„í‚¤í…ì²˜ë¥¼ ì œì‹œí•˜ë©° ë”¥ëŸ¬ë‹ ì—°êµ¬ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì—´ì—ˆìŠµë‹ˆë‹¤. ì—°ì‚°ì˜ ë³‘ë ¬í™”ë¥¼ í†µí•´ í•™ìŠµ ì†ë„ë¥¼ ë¹„ì•½ì ìœ¼ë¡œ ë†’ì˜€ì„ ë¿ë§Œ ì•„ë‹ˆë¼, ê¸°ì¡´ ëª¨ë¸ë“¤ì˜ ê³ ì§ˆì ì¸ ë¬¸ì œì˜€ë˜ ì¥ê±°ë¦¬ ì˜ì¡´ì„± ë¬¸ì œë¥¼ í•´ê²°í•¨ìœ¼ë¡œì¨ í˜„ì¬ GPTì™€ ê°™ì€ ì´ˆê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ë“¤ì´ íƒ„ìƒí•  ìˆ˜ ìˆëŠ” ê²°ì •ì ì¸ í† ëŒ€ë¥¼ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤."
    }
    
    # ë°ì´í„° ë¡œë“œ
    try:
        with open(subgraph_path, "r", encoding="utf-8") as f:
            dummy_subgraph = json.load(f)
        with open(user_info_path, "r", encoding="utf-8") as f:
            user_info_data = json.load(f)
        with open(paper_content_path, "r", encoding="utf-8") as f:
            paper_raw_data = json.load(f)
    except FileNotFoundError as e:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return

    curriculum_data = transform_subgraph_to_final_curriculum(dummy_subgraph, meta_data_input)

    # ì´ˆê¸° State êµ¬ì„± 
    initial_state = {
        "paper_name": paper_raw_data.get("title", "Unknown"),
        "paper_summary": paper_raw_data.get("abstract", ""),
        "initial_keywords": [n.get("keyword") for n in curriculum_data.get("nodes", [])],
        "paper_content": paper_raw_data,
        "user_info": user_info_data,
        "curriculum": curriculum_data,
        "tasks": [],
        "current_iteration_count": 0, # ì‹œì‘ ì¹´ìš´íŠ¸ 0
        "is_keyword_sufficient": False,
        "is_resource_sufficient": False,
        "needs_description_ids": [],
        "insufficient_resource_ids": [],
        "missing_concepts": [],
        "keyword_reasoning": "Init",
        "resource_reasoning": "Init",
        "keyword_expand_reason": ""
    }

   
    # StateGraph êµ¬ì„±
    workflow = StateGraph(CreateCurriculumOverallState)

    # ë…¸ë“œ ë“±ë¡
    workflow.add_node("orchestrator", curriculum_orchestrator_node)
    workflow.add_node("resource_discovery", resource_discovery_agent_node)
    workflow.add_node("paper_concept_alignment", paper_concept_alignment_node)
    workflow.add_node("concept_expansion", concept_expansion_node)
    workflow.add_node("curriculum_compose", curriculum_compose_node)
    
    # join ë…¸ë“œ ë“±ë¡
    workflow.add_node("join_results", join_parallel_results_node)

    # ì—£ì§€ ì—°ê²°
    workflow.add_edge(START, "orchestrator")
    
    # ë³‘ë ¬ ë…¸ë“œë“¤
    # ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ map ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ì™€ ì¼ì¹˜í•˜ëŠ” ë…¸ë“œë“¤ì´ ë™ì‹œ ì‹¤í–‰ë¨
    workflow.add_conditional_edges(
        "orchestrator",
        orchestrator_router,
        {
            "resource_discovery": "resource_discovery",
            "paper_concept_alignment": "paper_concept_alignment",
            "concept_expansion": "concept_expansion",
            "curriculum_compose": "curriculum_compose"
        }
    )

    
    # ì¼ì´ ëë‚˜ë©´ ë¬´ì¡°ê±´ Join ë…¸ë“œë¡œ ëª¨ì„
    workflow.add_edge("resource_discovery", "join_results")
    workflow.add_edge("paper_concept_alignment", "join_results")
    workflow.add_edge("concept_expansion", "join_results")
    workflow.add_edge("join_results", "orchestrator")

    # ì¢…ë£Œ ì²˜ë¦¬
    workflow.add_edge("curriculum_compose", END)

    # ì»´íŒŒì¼
    app = workflow.compile()

    # ì‹¤í–‰
    print("\nğŸš€ LangGraph ë³‘ë ¬ ì›Œí¬í”Œë¡œìš° ê°€ë™...")
    final_state = await app.ainvoke(initial_state)

    # ê²°ê³¼ ë¦¬í¬íŠ¸
    print("\n" + "="*50)
    print("ğŸ¯ ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸")
    print("="*50)
    print(f"ğŸ“Š ìµœì¢… ë°˜ë³µ íšŸìˆ˜: {final_state.get('current_iteration_count')}")
    print(f"âœ… ì „ì²´ ë¦¬ì†ŒìŠ¤ ì¶©ë¶„ì„±: {final_state.get('is_resource_sufficient')}")
    
    with open("langgraph_test_result_parallel_final.json", "w", encoding="utf-8") as f:
        json.dump(final_state, f, indent=2, ensure_ascii=False)
    print("\nâœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ.")

if __name__ == "__main__":
    asyncio.run(run_langgraph_workflow())