{
  "graph_meta": {
    "paper_id": "76162d4b-fe5f-49e4-b283-41480ba7e0eb",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "summarize": "BERT(Bidirectional Encoder Representations from Transformers) 논문은 양방향 트랜스포머 인코더를 기반으로 한 새로운 언어 표현 모델을 제안한다. 기존 단방향 언어 모델의 한계를 극복하기 위해 마스크된 언어 모델(MLM)과 다음 문장 예측(NSP) 작업을 통해 양방향 표현을 사전 학습한다. MLM은 입력 토큰의 15%를 무작위로 마스킹하고 주변 문맥을 기반으로 원본을 예측하며, NSP는 문장 쌍의 관계를 예측한다. 사전 학습된 BERT 모델은 추가 출력 계층만으로 다양한 NLP 작업(문장/토큰 수준 분류, 질의응답 등)에 미세 조정되어 최첨단 성능을 달성한다. GLUE, SQuAD, SWAG 등 11개 벤치마크에서 기존 방법 대비 큰 성능 향상을 보였으며, 모델 크기와 사전 학습의 중요성을 실험을 통해 입증한다."
  },
  "first_node_order": [
    "key-011",
    "key-012",
    "key-017",
    "key-005",
    "key-010",
    "key-016",
    "key-019",
    "key-020",
    "key-014",
    "key-013",
    "key-018"
  ],
  "nodes": [
    {
      "keyword_id": "key-001",
      "keyword": "Attention",
      "description": "Attention은 트랜스포머 모델의 핵심 메커니즘으로, BERT의 양방향 문맥 이해 능력을 가능하게 합니다. 트랜스포머 구조를 이해하려면 먼저 어텐션의 작동 원리를 숙지해야 하며, 이는 논문의 2.1절과 3.1절에서 설명되는 BERT의 사전 학습 방식과 직접 연결됩니다. 중요도: 9",
      "keyword_importance": 9,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-001",
          "resource_name": "What are Attention Mechanisms in Deep Learning? - freeCodeCamp",
          "url": "https://www.freecodecamp.org/news/what-are-attention-mechanisms-in-deep-learning/",
          "type": "web_doc",
          "resource_description": "Attention 메커니즘의 핵심 개념을 간결하게 설명하여 고급 학습자도 빠르게 복습하고 핵심 원리를 이해하는 데 유용합니다.",
          "difficulty": 3,
          "importance": 9,
          "study_load": 1,
          "is_necessary": true
        }
      ]
    },
    {
      "keyword_id": "key-002",
      "keyword": "Chain rule",
      "description": "Chain rule은 역전파 과정에서 그래디언트 계산의 수학적 기반이 됩니다. BERT의 학습 메커니즘을 이해하려면 신경망의 미분 기반 최적화 원리를 알아야 하며, 이는 3.2절의 미세 조정(fine-tuning) 섹션과 연결됩니다. 중요도: 6",
      "keyword_importance": 6,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-002",
          "resource_name": "Multivariable chain rule, simple version (article) - Khan Academy",
          "url": "https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/differentiating-vector-valued-functions/a/multivariable-chain-rule-simple-version",
          "type": "web_doc",
          "resource_description": "다변수 미적분학의 체인 룰을 단순화된 버전으로 설명하여 고급 학습자가 핵심 개념을 빠르게 복습하고 이해하는 데 유용합니다.",
          "difficulty": 4,
          "importance": 8,
          "study_load": 1.5,
          "is_necessary": false
        }
      ]
    },
    {
      "keyword_id": "key-003",
      "keyword": "Backpropagation",
      "description": "Backpropagation은 BERT 모델의 사전 학습과 미세 조정에 사용되는 핵심 최적화 알고리즘입니다. 3.1절의 MLM/NSP 학습 과정과 4절의 실험 결과 분석을 이해하려면 필수적인 개념입니다. 중요도: 8",
      "keyword_importance": 8,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-003",
          "resource_name": "Backpropagation in Neural Network - GeeksforGeeks",
          "url": "https://www.geeksforgeeks.org/machine-learning/backpropagation-in-neural-network/",
          "type": "web_doc",
          "resource_description": "Backpropagation의 핵심 개념을 간결하게 설명하여 고급 학습자가 빠르게 복습하고 핵심 원리를 재확인하기에 적합합니다.",
          "difficulty": 4,
          "importance": 9,
          "study_load": 1,
          "is_necessary": true
        }
      ]
    },
    {
      "keyword_id": "key-004",
      "keyword": "Fine-tuning",
      "description": "Fine-tuning은 사전 학습된 BERT 모델을 다운스트림 NLP 작업에 적용하는 핵심 단계입니다. 논문의 3.2절과 4.1절에서 다양한 태스크에 대한 성능 평가 방법을 이해하려면 이 개념이 선행되어야 합니다. 중요도: 9",
      "keyword_importance": 9,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-004",
          "resource_name": "Understanding Fine-Tuning in AI and ML | Databricks",
          "url": "https://www.databricks.com/glossary/fine-tuning",
          "type": "web_doc",
          "resource_description": "Fine-tuning의 개념과 파라미터 효율적 기법(PEFT)을 포함해 기업 데이터 적용 사례를 명확히 설명하여, 마스터 수준 학습자가 생성형 AI 모델 최적화 전략을 빠르게 이해하는 데 유용합니다.",
          "difficulty": 4,
          "importance": 8,
          "study_load": 1.5,
          "is_necessary": true
        }
      ]
    },
    {
      "keyword_id": "key-005",
      "keyword": "NLP tasks",
      "description": "NLP tasks는 BERT의 성능을 평가하는 기준입니다. GLUE, SQuAD 등 11개 벤치마크(4.2절)를 이해하려면 분류, 추론, 질의응답 등 기본 NLP 태스크에 대한 지식이 필요합니다. 중요도: 7",
      "keyword_importance": 7,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-005",
          "resource_name": "Natural Language Processing (NLP) Tasks - GeeksforGeeks",
          "url": "https://www.geeksforgeeks.org/machine-learning/natural-language-processing-nlp-tasks/",
          "type": "web_doc",
          "resource_description": "NLP의 핵심 태스크를 간결하게 정리하여 실무 적용 가능한 기초 지식을 빠르게 습득할 수 있는 자료",
          "difficulty": 3,
          "importance": 8,
          "study_load": 1.5,
          "is_necessary": false
        }
      ]
    },
    {
      "keyword_id": "key-006",
      "keyword": "BERT",
      "description": "BERT는 논문의 핵심 주제로, 트랜스포머 기반 양방향 언어 모델의 구조와 학습 방법을 포함합니다. 모든 섹션(특히 2-4절)에서 다루는 내용을 이해하려면 이 개념에 대한 체계적인 학습이 선행되어야 합니다. 중요도: 10",
      "keyword_importance": 10,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-006",
          "resource_name": "BERT 개념 정리 (특징/구조/동작 방식/종류/장점/BERT 모델 설명)",
          "url": "https://happy-obok.tistory.com/23",
          "type": "web_doc",
          "resource_description": "BERT의 핵심 개념, 구조, 학습 방법, 활용 분야를 체계적으로 정리하여 자연어 처리 분야의 필수 지식을 효율적으로 습득할 수 있는 자료",
          "difficulty": 3,
          "importance": 9,
          "study_load": 1.5,
          "is_necessary": true
        }
      ]
    },
    {
      "keyword_id": "key-007",
      "keyword": "Transformer",
      "description": "Transformer는 BERT의 기반 아키텍처입니다. Self-attention 메커니즘과 멀티헤드 어텐션(2.1절)을 이해하려면 트랜스포머 인코더 구조를 먼저 학습해야 합니다. 중요도: 9",
      "keyword_importance": 9,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-007",
          "resource_name": "An intuitive overview of the Transformer architecture - Medium",
          "url": "https://medium.com/@roberto.g.infante/an-intuitive-overview-of-the-transformer-architecture-6a88ccc88171",
          "type": "web_doc",
          "resource_description": "Transformer 아키텍처의 핵심 개념을 직관적으로 설명하여 복잡한 구조를 쉽게 이해할 수 있도록 돕는 자료",
          "difficulty": 3,
          "importance": 8,
          "study_load": 1,
          "is_necessary": true
        },
        {
          "resource_id": "res-013",
          "resource_name": "A Deep Dive into the Self-Attention Mechanism of Transformers",
          "url": "https://medium.com/analytics-vidhya/a-deep-dive-into-the-self-attention-mechanism-of-transformers-fe943c77e654",
          "type": "web_doc",
          "resource_description": "Self-Attention 메커니즘의 핵심 원리와 수학적 구조를 명확히 설명하여 Transformer 아키텍처의 필수 개념을 체계적으로 이해할 수 있게 합니다.",
          "difficulty": 5,
          "importance": 9,
          "study_load": 1.5,
          "is_necessary": false
        }
      ]
    },
    {
      "keyword_id": "key-008",
      "keyword": "Language representation model",
      "description": "Language representation model은 BERT가 해결하고자 하는 문제의 배경입니다. 1절과 2절에서 설명하는 기존 모델(ELMo, GPT)과의 비교를 이해하려면 언어 표현 모델의 발전 과정을 알아야 합니다. 중요도: 7",
      "keyword_importance": 7,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-008",
          "resource_name": "Language Representation Models: An Overview - MDPI",
          "url": "https://www.mdpi.com/1099-4300/23/11/1422",
          "type": "paper",
          "resource_description": "언어 표현 모델의 핵심 개념과 발전 과정을 체계적으로 정리하여, 자연어 처리 분야의 이론적 기반을 탄탄히 다질 수 있는 논문입니다.",
          "difficulty": 6,
          "importance": 8,
          "study_load": 2.5,
          "is_necessary": false
        }
      ]
    },
    {
      "keyword_id": "key-009",
      "keyword": "Textual entailment",
      "description": "Textual entailment는 BERT가 해결하는 주요 NLP 태스크 중 하나입니다. MultiNLI 데이터셋(4.2절) 실험 결과를 분석하려면 텍스트 함의 관계에 대한 이해가 필요합니다. 중요도: 6",
      "keyword_importance": 6,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-009",
          "resource_name": "Textual entailment - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Textual_entailment",
          "type": "web_doc",
          "resource_description": "텍스트 함의(TE)와 자연어 추론(NLI)의 기본 개념 및 관련 NLP 기술을 폭넓게 이해할 수 있는 위키피디아 문서로, 핵심 용어 정의와 연계 분야 파악이 용이합니다.",
          "difficulty": 4,
          "importance": 8,
          "study_load": 1.5,
          "is_necessary": false
        }
      ]
    },
    {
      "keyword_id": "key-010",
      "keyword": "Question answering",
      "description": "Question answering은 BERT의 성능을 평가하는 핵심 태스크입니다. SQuAD v1.1/v2.0 실험(4.2절)을 이해하려면 질의응답 시스템의 기본 원리를 알아야 합니다. 중요도: 8",
      "keyword_importance": 8,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-010",
          "resource_name": "Context-Based Question Answering Models - Emergent Mind",
          "url": "https://www.emergentmind.com/topics/context-based-question-answering-cbqa-models",
          "type": "web_doc",
          "resource_description": "이 자료는 그래프 기반 QA 모델과 구조화된 컨텍스트 융합 기법을 포함해 최신 CBQA 모델 아키텍처와 손실 함수 설계를 심층적으로 다루어, 고급 학습자에게 필수적인 이론적 기반을 제공합니다.",
          "difficulty": 7,
          "importance": 8,
          "study_load": 2.5,
          "is_necessary": false
        }
      ]
    },
    {
      "keyword_id": "key-011",
      "keyword": "Masked Language Model",
      "description": "Masked Language Model은 BERT 사전 학습의 핵심 전략입니다. 3.1절에서 설명하는 양방향 문맥 학습 메커니즘과 직접 연결되며, 모델 성능 향상의 주요 원인입니다. 중요도: 9",
      "keyword_importance": 9,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-011",
          "resource_name": "Masked Language Modeling with Python Transformer Architecture.md",
          "url": "https://github.com/xbeat/Machine-Learning/blob/main/Masked%20Language%20Modeling%20with%20Python%20Transformer%20Architecture.md",
          "type": "web_doc",
          "resource_description": "Transformer 아키텍처의 핵심 기술인 MLM을 파이썬 구현과 함께 설명하여 고급 학습자가 빠르게 개념을 복습하고 적용할 수 있도록 구성된 문서입니다.",
          "difficulty": 4,
          "importance": 9,
          "study_load": 1.5,
          "is_necessary": true
        }
      ]
    },
    {
      "keyword_id": "key-012",
      "keyword": "Next Sentence Prediction",
      "description": "Next Sentence Prediction은 BERT의 문장 수준 사전 학습 목표입니다. 3.1절과 4.3절에서 분석하는 문장 관계 이해 능력과 연결되며, 다운스트림 태스크 성능에 영향을 미칩니다. 중요도: 7",
      "keyword_importance": 7,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-012",
          "resource_name": "BERT - choice - 티스토리",
          "url": "https://choice-life.tistory.com/25",
          "type": "web_doc",
          "resource_description": "BERT의 핵심 학습 과제인 Next Sentence Prediction의 기본 개념을 간결하게 설명하여, 고급 학습자가 빠르게 복습할 수 있는 유용한 자료입니다.",
          "difficulty": 3,
          "importance": 8,
          "study_load": 0.5,
          "is_necessary": true
        }
      ]
    },
    {
      "keyword_id": "key-013",
      "keyword": "Gradient descent",
      "description": "Gradient descent는 BERT의 사전 학습 및 미세 조정 과정에서 모델 파라미터를 최적화하는 핵심 알고리즘입니다. Backpropagation과 연결되어 손실 함수 최소화를 위한 가중치 업데이트 방식을 이해해야 하며, 특히 대규모 트랜스포머 모델의 학습 효율성을 파악하는 데 필수적입니다. 논문의 실험 결과 해석에도 영향을 미칩니다.",
      "keyword_importance": 8,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-014",
          "resource_name": "[PDF] Lecture 6: September 12 6.1 Gradient Descent: Convergence Analysis",
          "url": "https://www.stat.cmu.edu/~ryantibs/convexopt-F13/scribes/lec6.pdf",
          "type": "paper",
          "resource_description": "경사 하강법의 수렴 분석을 엄밀하게 다루며, 고정 스텝 사이즈와 백트래킹 라인 서치의 이론적 성능을 비교하는 핵심 내용을 제공하여 최적화 이론 이해에 필수적입니다.",
          "difficulty": 7,
          "importance": 9,
          "study_load": 2.5,
          "is_necessary": false
        }
      ]
    },
    {
      "keyword_id": "key-014",
      "keyword": "Neural network",
      "description": "Neural network는 BERT의 기본 구조를 구성하는 필수 개념입니다. 트랜스포머 인코더가 다층 신경망으로 구현되며, 순전파/역전파 메커니즘과 활성화 함수 등의 기본 원리를 이해해야 모델 동작을 분석할 수 있습니다. Attention 메커니즘과 결합되는 방식을 학습하는 데 선행 지식이 됩니다.",
      "keyword_importance": 9,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-015",
          "resource_name": "Mathematical Foundations of Neural Networks - CNS Tech Lab",
          "url": "http://techlab.bu.edu/resources/cat/C52/index.html",
          "type": "web_doc",
          "resource_description": "신경망의 수학적 기반을 미분방정식 등 고급 수학 방법으로 체계적으로 설명하여, 석사 수준 학습자에게 이론적 이해를 심화시키는 데 유용합니다.",
          "difficulty": 7,
          "importance": 9,
          "study_load": 3.5,
          "is_necessary": false
        }
      ]
    },
    {
      "keyword_id": "key-015",
      "keyword": "Word embeddings",
      "description": "Word embeddings는 BERT의 입력 표현과 직접적으로 연결됩니다. 토큰을 벡터로 변환하는 과정과 문맥 독립적 임베딩(예: Word2Vec)과의 차이점을 이해해야 양방향 표현 학습의 혁신성을 파악할 수 있습니다. 언어 표현 모델(Language representation model) 섹션의 기반이 됩니다.",
      "keyword_importance": 7,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-016",
          "resource_name": "The Ultimate Guide to Word Embeddings",
          "url": "https://neptune.ai/blog/word-embeddings-guide",
          "type": "web_doc",
          "resource_description": "워드 임베딩의 수학적 정의와 기본 개념을 명확히 설명하여 자연어 처리 분야의 핵심 기술을 체계적으로 이해하는 데 도움을 줍니다.",
          "difficulty": 4,
          "importance": 8,
          "study_load": 1.5,
          "is_necessary": false
        },
        {
          "resource_id": "res-018",
          "resource_name": "A Comparative Analysis of Static Word Embeddings for Hungarian",
          "url": "https://arxiv.org/html/2505.07809v1",
          "type": "paper",
          "resource_description": "헝가리어 정적 임베딩 추출 방법 중 X2Static의 우수성을 분석한 논문으로, BERT 기반 모델 비교 연구를 통해 고급 NLP 기술 이해에 도움을 줍니다.",
          "difficulty": 7,
          "importance": 8,
          "study_load": 2.5,
          "is_necessary": false
        }
      ]
    },
    {
      "keyword_id": "key-016",
      "keyword": "Natural language inference",
      "description": "Natural language inference(NLI)는 BERT가 성능을 입증한 주요 NLP 태스크 중 하나입니다. MultiNLI 데이터셋 실험 결과를 이해하려면 NLI의 개념(문장 간 함의 관계 판단)과 평가 방식을 알아야 합니다. Textual entailment와 동일한 개념으로, GLUE 벤치마크 분석 시 필수적입니다.",
      "keyword_importance": 6,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-017",
          "resource_name": "16.4. Natural Language Inference and the Dataset",
          "url": "https://d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html",
          "type": "web_doc",
          "resource_description": "SNLI 데이터셋과 자연어 추론(NLI) 태스크를 구현하는 코드 예제를 제공하여, 고급 학습자가 NLI 모델 개발 및 평가를 위한 실용적인 이해를 높이는 데 유용합니다.",
          "difficulty": 4,
          "importance": 8,
          "study_load": 1.5,
          "is_necessary": false
        }
      ]
    },
    {
      "keyword_id": "key-017",
      "keyword": "Self-attention",
      "description": "Self-attention은 트랜스포머 모델의 핵심 메커니즘으로, BERT의 양방향 문맥 이해를 가능하게 합니다. 논문에서 모든 레이어가 좌우 문맥을 동시에 고려하는 구조를 설명하려면 self-attention의 작동 원리를 먼저 이해해야 합니다. 트랜스포머 섹션(key-007)과 직접 연결되며, BERT의 사전 학습 전략(MLM/NSP) 구현의 기반이 됩니다.",
      "keyword_importance": 9,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-019",
          "resource_name": "How Attention Mechanism Works in Transformer Architecture",
          "url": "https://www.youtube.com/watch?v=KMHkbXzHn7s",
          "type": "video",
          "resource_description": "트랜스포머 아키텍처의 핵심 메커니즘인 self-attention의 작동 원리를 시각적으로 이해하기에 적합한 영상 자료입니다.",
          "difficulty": 4,
          "importance": 9,
          "study_load": 1,
          "is_necessary": true
        }
      ]
    },
    {
      "keyword_id": "key-018",
      "keyword": "Multi-head attention",
      "description": "Multi-head attention은 단일 attention 헤드의 한계를 극복하기 위해 BERT에 적용된 기술로, 다양한 표현 하위공간을 학습합니다. 트랜스포머 아키텍처(key-007)의 핵심 구성 요소이며, BERT의 성능 향상 요인을 분석할 때 반드시 이해해야 하는 개념입니다. 모델 구조 섹션과 사전 학습 효율성 논의에 직접 연관됩니다.",
      "keyword_importance": 8,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-020",
          "resource_name": "[2106.09650] Multi-head or Single-head? An Empirical Comparison ...",
          "url": "https://arxiv.org/abs/2106.09650",
          "type": "paper",
          "resource_description": "멀티헤드 어텐션과 싱글헤드 어텐션의 구조적 차이와 학습 안정성 효과를 실증적으로 비교한 논문으로, 트랜스포머 아키텍처 최적화에 대한 심층적 이해를 제공합니다.",
          "difficulty": 7,
          "importance": 8,
          "study_load": 2.5,
          "is_necessary": false
        }
      ]
    },
    {
      "keyword_id": "key-019",
      "keyword": "Positional encoding",
      "description": "Positional encoding은 트랜스포머가 단어 순서 정보를 보존하는 메커니즘으로, BERT의 입력 처리 방식을 이해하는 데 필수적입니다. Self-attention(key-017)과 트랜스포머(key-007) 학습 전에 선행되어야 하며, 토큰 위치 정보가 모델 성능에 미치는 영향을 분석하는 데 기반이 됩니다.",
      "keyword_importance": 7,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-021",
          "resource_name": "What is Positional Encoding? | IBM",
          "url": "https://www.ibm.com/think/topics/positional-encoding",
          "type": "web_doc",
          "resource_description": "트랜스포머 모델의 핵심 요소인 Positional Encoding의 수학적 구현과 시각화 방법을 코드와 함께 설명하여 고급 학습자가 개념을 명확히 이해하는 데 도움을 줍니다.",
          "difficulty": 4,
          "importance": 8,
          "study_load": 1,
          "is_necessary": true
        }
      ]
    },
    {
      "keyword_id": "key-020",
      "keyword": "Tokenization",
      "description": "Tokenization은 텍스트를 서브워드 단위로 분할하는 과정으로, BERT의 WordPiece 토크나이저 적용 방식을 이해하려면 선행되어야 합니다. 언어 표현 모델(key-008)과 단어 임베딩(key-015) 학습의 입력 단계를 구성하며, MLM 작업 설계 및 실험 결과 해석에 직접적인 영향을 미칩니다.",
      "keyword_importance": 6,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-022",
          "resource_name": "Tokenization in NLP. Why It Matters and How It Works - Md Ismail Sojal",
          "url": "https://0xsojalsec.medium.com/tokenization-in-nlp-a5aebe0232ad",
          "type": "web_doc",
          "resource_description": "토큰화의 기본 개념과 다양한 유형(문자, 단어, 서브워드)을 명확히 설명하며, BPE와 같은 실제 적용 사례를 포함해 NLP 모델 학습의 필수 기초를 체계적으로 이해할 수 있는 자료",
          "difficulty": 3,
          "importance": 9,
          "study_load": 1.5,
          "is_necessary": true
        }
      ]
    }
  ],
  "edges": [
    {
      "start": "key-001",
      "end": "key-007"
    },
    {
      "start": "key-007",
      "end": "key-006"
    },
    {
      "start": "key-002",
      "end": "key-003"
    },
    {
      "start": "key-003",
      "end": "key-004"
    },
    {
      "start": "key-006",
      "end": "76162d4b-fe5f-49e4-b283-41480ba7e0eb"
    },
    {
      "start": "key-004",
      "end": "76162d4b-fe5f-49e4-b283-41480ba7e0eb"
    },
    {
      "start": "key-010",
      "end": "76162d4b-fe5f-49e4-b283-41480ba7e0eb"
    },
    {
      "start": "key-009",
      "end": "76162d4b-fe5f-49e4-b283-41480ba7e0eb"
    },
    {
      "start": "key-005",
      "end": "76162d4b-fe5f-49e4-b283-41480ba7e0eb"
    },
    {
      "start": "key-008",
      "end": "76162d4b-fe5f-49e4-b283-41480ba7e0eb"
    },
    {
      "start": "key-007",
      "end": "76162d4b-fe5f-49e4-b283-41480ba7e0eb"
    },
    {
      "start": "key-011",
      "end": "76162d4b-fe5f-49e4-b283-41480ba7e0eb"
    },
    {
      "start": "key-012",
      "end": "76162d4b-fe5f-49e4-b283-41480ba7e0eb"
    },
    {
      "start": "key-014",
      "end": "key-003"
    },
    {
      "start": "key-013",
      "end": "key-002"
    },
    {
      "start": "key-016",
      "end": "key-009"
    },
    {
      "start": "key-015",
      "end": "key-008"
    },
    {
      "start": "key-018",
      "end": "key-007"
    },
    {
      "start": "key-019",
      "end": "key-007"
    },
    {
      "start": "key-017",
      "end": "key-001"
    },
    {
      "start": "key-020",
      "end": "key-015"
    }
  ]
}