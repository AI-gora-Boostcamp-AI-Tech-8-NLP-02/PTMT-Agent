{
  "graph": {
    "nodes": {
      "keywords": [
        {
          "name": "Transformer (deep learning)",
          "link": "https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29",
          "alias": [
            "transformer",
            "Transformer architecture",
            "transformer model",
            "transformer neural network",
            "transformer architecture",
            "transformer deep learning",
            "Transformer",
            "transformer network",
            "Transformer Architecture",
            "transformer_architecture",
            "transformer architecture model",
            "Transformer architectures",
            "transformer architectures",
            "Transformer Architectures",
            "transformer models",
            "transformer-based architectures",
            "Transformer language model",
            "transformer language model",
            "transformer-based language model",
            "transformer LM",
            "transformer language modelling",
            "transformer lm",
            "Transformer models",
            "TransformerModels",
            "transformer-models",
            "Transformer-based language models",
            "transformer-based language models",
            "transformer language models",
            "transformer-based models",
            "transformer-based NLP models",
            "transformer-based neural language models",
            "Transformer-based model",
            "transformer based model",
            "transformer-based model",
            "transformer-based neural network",
            "transformer model architecture",
            "Transformer-based models",
            "transformer based models",
            "transformer-based neural networks",
            "transformer-based systems",
            "Transformer-based neural architecture",
            "transformer neural architecture",
            "transformer-based network",
            "transformer-based deep architecture",
            "Transformers",
            "transformers",
            "Transformers library",
            "transformers library",
            "TransformersLibrary",
            "transformer library",
            "transformers lib",
            "transformer encoder-decoder architecture",
            "transformer encoder decoder architecture",
            "transformer encoder-decoder",
            "transformer encoder decoder",
            "transformer encoder-decoder model",
            "encoder-decoder transformer",
            "language model transformer",
            "lm transformer",
            "transformer model for language",
            "Transformer Model",
            "transformermodel",
            "transformer-model",
            "transformerModel",
            "Transformer Models",
            "transformer_models",
            "transformermodelss",
            "transformer-based architecture",
            "transformer-based networks",
            "transformer based networks",
            "transformer networks"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:2710",
          "categories": [
            "19th-century inventions|British inventions|Electric power conversion|Electric transformers|Electrical engineering|Hungarian inventions",
            "2017 in artificial intelligence|Google software|Neural network architectures",
            "Fiction about alien visitations|Fiction about immortality|Fictional extraterrestrial robots|Films about shapeshifting|Hasbro franchises|Mass media franchises|Mass media franchises introduced in 1984|Military fiction|Science fiction franchises|Super robot anime and manga|Takara Tomy franchises|Television series about shapeshifting|Transformers (franchise)"
          ]
        },
        {
          "name": "Attention (machine learning)",
          "link": "https://en.wikipedia.org/wiki/Attention_%28machine_learning%29",
          "alias": [
            "attention",
            "Attention",
            "attention mechanism",
            "Attention mechanism",
            "attention model",
            "Attention model",
            "self-attention",
            "Self-attention",
            "scaled dot-product attention",
            "Scaled dot-product attention",
            "Self-attention mechanisms",
            "self attention mechanisms",
            "self attention",
            "self-attention mechanisms",
            "selfattention",
            "SelfAttention",
            "self attention mechanism",
            "self-attention mechanism"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3986",
          "categories": [
            "Machine learning",
            "Disambiguation pages"
          ]
        },
        {
          "name": "attention-only mechanism",
          "link": NaN,
          "alias": [
            "attention only mechanism",
            "attention-only model",
            "attentiononly mechanism",
            "attention only model",
            "attention only architecture"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:9544",
          "categories": []
        },
        {
          "name": "encoder-decoder architecture",
          "link": NaN,
          "alias": [
            "encoder decoder architecture",
            "encoder-decoder",
            "encoder decoder",
            "enc-dec architecture",
            "encoder decoder model",
            "enc-dec",
            "encoder-decoder attention",
            "encoder decoder attention",
            "attention encoder decoder",
            "attention encoder-decoder",
            "encoder-decoder framework",
            "encoder decoder framework",
            "encoder-decoder model",
            "encoder-decoder architecture",
            "enc-dec framework"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:11362",
          "categories": []
        },
        {
          "name": "parallelizable training",
          "link": NaN,
          "alias": [
            "parallelizabletraining",
            "parallelizable-training"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:14531",
          "categories": []
        },
        {
          "name": "single-model state-of-the-art",
          "link": NaN,
          "alias": [
            "single model state of the art",
            "single-model SOTA",
            "single model SOTA",
            "single model state-of-the-art model",
            "SOTA single model"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:15883",
          "categories": []
        },
        {
          "name": "Machine translation",
          "link": "https://en.wikipedia.org/wiki/Machine_Translation",
          "alias": [
            "machine translation",
            "MT",
            "MachineTranslation",
            "machine translation system",
            "translation machine",
            "MT system",
            "Machine translation"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:593",
          "categories": [
            "Applications of artificial intelligence|Automation software|Computational linguistics|Computer-assisted translation|Machine translation|Tasks of natural language processing"
          ]
        },
        {
          "name": "Rnn (software)",
          "link": "https://en.wikipedia.org/wiki/Rnn_%28software%29",
          "alias": [
            "rnn",
            "RNN",
            "rnn model",
            "recurrent neural network",
            "Recurrent Neural Network",
            "RNN model"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1838",
          "categories": [
            "Deep learning software",
            "Free R (programming language) software",
            "Free science software",
            "Free statistical software",
            "Open-source artificial intelligence",
            "R (programming language)",
            "R scientific libraries"
          ]
        },
        {
          "name": "Seq2seq",
          "link": "https://en.wikipedia.org/wiki/Seq2seq",
          "alias": [
            "seq2seq",
            "Seq2Seq",
            "sequence-to-sequence",
            "sequence to sequence",
            "seq 2 seq",
            "seq2 seq",
            "Seq2seq"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:2124",
          "categories": [
            "2014 in artificial intelligence",
            "Artificial neural networks",
            "Natural language processing",
            "2014 in artificial intelligence|Artificial neural networks|Natural language processing"
          ]
        },
        {
          "name": "Convolutional neural network",
          "link": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
          "alias": [
            "convolutional neural network",
            "CNN",
            "convnet",
            "convolutional neural net",
            "convolutional net",
            "CNN model"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4771",
          "categories": [
            "Computational neuroscience",
            "Computer vision",
            "Neural network architectures"
          ]
        },
        {
          "name": "Pretrained Encoder-Decoder Models",
          "link": NaN,
          "alias": [
            "pretrained encoder decoder models",
            "pretrained encoder-decoder models",
            "pretrained encoder decoder",
            "encoder decoder models",
            "pretrained enc-dec models",
            "encoder-decoder models",
            "encoder-decoder",
            "encoder_decoder_models",
            "encoder decoder",
            "enc-dec models",
            "enc-dec",
            "encoder-decoder network",
            "encoder decoder network",
            "encoder_decoder",
            "enc-dec network",
            "encoder-decoder networks",
            "encoder decoder networks",
            "encoder_decoder networks",
            "enc-dec networks",
            "encoder–decoder architecture",
            "encoder-decoder architecture",
            "encoder decoder architecture",
            "enc-dec architecture",
            "retriever-ranker encoder-decoder architectures",
            "retriever-ranker",
            "retriever ranker",
            "retriever ranker encoder decoder",
            "retriever ranker encoder decoder architectures"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:8397",
          "categories": []
        },
        {
          "name": "Multi-head Self-Attention",
          "link": NaN,
          "alias": [
            "multi-head self-attention",
            "Multi-Head Self-Attention",
            "multi head self attention",
            "MHSA",
            "multi-head attention",
            "self-attention with multiple heads"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:8101",
          "categories": []
        },
        {
          "name": "novel positional encoding scheme",
          "link": NaN,
          "alias": [
            "novel positional encoding",
            "positional encoding scheme",
            "novel position encoding",
            "positional encoding",
            "position encoding scheme"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:14291",
          "categories": []
        },
        {
          "name": "Backpropagation",
          "link": "https://en.wikipedia.org/wiki/Backpropagation",
          "alias": [
            "backpropagation",
            "Backprop",
            "backprop",
            "BP",
            "back propagation",
            "back-propagation",
            "Backpropagation",
            "Backpropagation algorithm",
            "Binary pattern",
            "binary pattern",
            "binary-pattern",
            "BinaryPattern",
            "binarypattern",
            "binary pattern recognition",
            "arterial hypertension",
            "high blood pressure",
            "hypertension",
            "high BP",
            "HTN"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4105",
          "categories": [
            "Artificial neural networks",
            "Machine learning algorithms",
            "Artificial neural networks|Machine learning algorithms",
            "Hypertension|Medical conditions related to obesity"
          ]
        },
        {
          "name": "Gradient",
          "link": "https://en.wikipedia.org/wiki/Gradient",
          "alias": [
            "gradient",
            "grad",
            "gradient descent",
            "gradients",
            "Gradients",
            "grads",
            "gradient vector"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:5908",
          "categories": [
            "Differential calculus",
            "Differential operators",
            "Generalizations of the derivative",
            "Linear operators in calculus",
            "Rates",
            "Vector calculus",
            "Differential calculus|Differential operators|Generalizations of the derivative|Linear operators in calculus|Rates|Vector calculus"
          ]
        },
        {
          "name": "Differentiable function",
          "link": "https://en.wikipedia.org/wiki/Differentiable_function",
          "alias": [
            "differentiable function",
            "differentiable func",
            "diff function",
            "differentiable",
            "diff'ble function",
            "differentiable mapping"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:5134",
          "categories": [
            "Multivariable calculus",
            "Smooth functions"
          ]
        },
        {
          "name": "Chain rule",
          "link": "https://en.wikipedia.org/wiki/Chain_rule",
          "alias": [
            "chain rule",
            "Chain Rule",
            "chainrule",
            "chain-rule",
            "Chainrule",
            "calculus chain rule"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4419",
          "categories": [
            "Differentiation rules",
            "Theorems in calculus",
            "Theorems in mathematical analysis"
          ]
        },
        {
          "name": "Partial derivative",
          "link": "https://en.wikipedia.org/wiki/Partial_derivative",
          "alias": [
            "partial deriv",
            "partial derivative",
            "partial derivativ",
            "partial diff",
            "partial differentiation",
            "δf/δx"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1365",
          "categories": [
            "Differential operators",
            "Multivariable calculus"
          ]
        },
        {
          "name": "Attention Is All You Need",
          "link": "https://en.wikipedia.org/wiki/Attention_Is_All_You_Need",
          "alias": [
            "attention is all you need",
            "AttentionIsAllYouNeed",
            "attention all you need",
            "attention is all you need paper",
            "transformer paper",
            "all you need is attention"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3987",
          "categories": [
            "2017 documents",
            "2017 in artificial intelligence",
            "Artificial intelligence papers",
            "Google"
          ]
        },
        {
          "name": "Parallel text",
          "link": "https://en.wikipedia.org/wiki/Parallel_corpora",
          "alias": [
            "parallel corpora",
            "parallel corpora",
            "parallel-corpus",
            "parallel corpus",
            "parallel corpora",
            "parallel corpus",
            "parallel corpora",
            "parallel corpus",
            "parallelcorpus",
            "parallel texts",
            "paralleltext",
            "parallel-texts",
            "parallel text",
            "parallel texts corpus",
            "paralleltext corpus"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1348",
          "categories": [
            "Corpus linguistics|Language acquisition|Translation databases"
          ]
        },
        {
          "name": "Natural language processing",
          "link": "https://en.wikipedia.org/wiki/Natural_Language_Processing",
          "alias": [],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1040",
          "categories": []
        },
        {
          "name": "Recurrent neural network",
          "link": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
          "alias": [
            "RNN",
            "recurrent neural network",
            "rnn",
            "recurrent neural net"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1725",
          "categories": [
            "Neural network architectures"
          ]
        },
        {
          "name": "Backpropagation through time",
          "link": "https://en.wikipedia.org/wiki/Backpropagation_through_time",
          "alias": [
            "bptt",
            "BPTT",
            "backpropagation through time",
            "backprop through time",
            "bptt algorithm",
            "backpropagation through retrieval",
            "backprop through retrieval"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4110",
          "categories": [
            "Artificial neural networks"
          ]
        },
        {
          "name": "Convolution",
          "link": "https://en.wikipedia.org/wiki/Convolution",
          "alias": [],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4770",
          "categories": [
            "Bilinear maps",
            "Feature detection (computer vision)",
            "Fourier analysis",
            "Functional analysis",
            "Image processing"
          ]
        },
        {
          "name": "Neural network (machine learning)",
          "link": "https://en.wikipedia.org/wiki/Artificial_Neural_Network",
          "alias": [
            "artificial neural network",
            "ANN",
            "artificial neural networks",
            "ann",
            "neural network",
            "neural networks",
            "artificial neural net"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3917",
          "categories": []
        },
        {
          "name": "Multilayer perceptron",
          "link": "https://en.wikipedia.org/wiki/Multilayer_perceptron",
          "alias": [
            "multilayer perceptron",
            "MLP",
            "multilayer perceptron neural network",
            "multi-layer perceptron",
            "MLP network",
            "multilayer perceptron model",
            "Multilayer perceptron",
            "multi layer perceptron",
            "mlp",
            "multi-layer perceptron interaction",
            "mlp interaction",
            "multi layer perceptron interaction",
            "multilayer perceptron interaction",
            "MLP interaction"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:959",
          "categories": [
            "Classification algorithms",
            "Neural network architectures",
            "Classification algorithms|Neural network architectures"
          ]
        },
        {
          "name": "Artificial neural network",
          "link": "https://en.wikipedia.org/wiki/Artificial_neural_network",
          "alias": [
            "artificial neural network",
            "ANN",
            "artificial neural network model",
            "neural network",
            "artificial neural networks",
            "ann"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3095",
          "categories": []
        },
        {
          "name": "Neural backpropagation",
          "link": "https://en.wikipedia.org/wiki/Neural_backpropagation",
          "alias": [
            "neural backpropagation",
            "neural back propagation",
            "backpropagation",
            "back propagation",
            "neural bp",
            "bp"
          ],
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1092",
          "categories": [
            "Computational neuroscience",
            "Neural circuitry",
            "Neuroscience"
          ]
        }
      ],
      "papers": [
        {
          "name": "Transformers in Vision: A Survey",
          "description": "이 논문은 컴퓨터 비전 분야에서 Transformer 모델의 기초 개념과 응용을 종합적으로 조망하며, 이미지 분류, 객체 검출, 행동 인식, 세그멘테이션 등 다양한 작업에 대한 성능과 한계를 비교 분석한다.",
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:17634",
          "url": "https://www.semanticscholar.org/paper/3a906b77fa218adc171fecb28bb81c24c14dcc7b"
        },
        {
          "name": "A survey of the recent architectures of deep convolutional neural networks",
          "description": "최근에 보고된 깊은 컨볼루셔널 신경망 아키텍처의 내적 분류를 위해 공간, 깊이, 다중 경로, 폭, 특징맵 활용, 채널 강화 및 주의 메커니즘의 7가지 범주로 분류한 조사 보고서이다.",
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:17806",
          "url": "https://www.semanticscholar.org/paper/8452941a175c899628c679523da952b18f63e335"
        },
        {
          "name": "EnlightenGAN: Deep Light Enhancement Without Paired Supervision",
          "description": "낮은 조명 이미지 강화를 위해 쌍이 없는 학습을 가능하게 한 EnlightenGAN은 입력 이미지 자체의 정보를 활용한 자기 정규화 손실과 전역-국소 판별자 구조를 통해 우수한 성능을 발휘한다.",
          "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18015",
          "url": "https://www.semanticscholar.org/paper/228a801801abd28b58db1de75cfe6885f7d47bb7"
        }
      ]
    },
    "edges": {
      "IN": [
        {
          "reason": "The paper explicitly contrasts its attention-only architecture with recurrent models, making knowledge of RNNs essential to understanding the motivation and innovation. Without familiarity with RNNs, the paper's key contribution is difficult to grasp.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1838",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18188"
        },
        {
          "reason": "The paper builds upon existing seq2seq models with attention mechanisms, making familiarity with the concept essential for understanding the context and motivation behind the proposed architecture.",
          "strength": 0.8,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:2124",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18188"
        },
        {
          "reason": "The paper introduces a new architecture entirely based on attention mechanisms, making it a core component of the model. Without understanding attention, the paper's foundational concept cannot be grasped.",
          "strength": 0.95,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3986",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18188"
        },
        {
          "reason": "The paper contrasts its attention-only architecture with models that use convolutional neural networks, making knowledge of CNNs helpful for understanding the context and motivation. However, the core of the paper does not depend on understanding CNNs.",
          "strength": 0.7,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4771",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18188"
        },
        {
          "reason": "The paper focuses on machine translation tasks, and the proposed model is evaluated specifically on translation benchmarks. The results and experiments are centered around improving performance in machine translation, making it a core prerequisite for understanding the context and goals of the paper.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:593",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18188"
        },
        {
          "reason": "The paper directly builds upon and contrasts with encoder-decoder models, making them a foundational prerequisite for understanding the proposed architecture's innovation and context.",
          "strength": 0.95,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:8397",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18188"
        }
      ],
      "REF_BY": [],
      "PREREQ": [
        {
          "reason": "Machine translation is a core application of natural language processing, requiring understanding of linguistic structures and text generation.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1040",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:593"
        },
        {
          "reason": "Neural backpropagation is essential for training artificial neural networks, as it enables gradient computation and weight updates.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1092",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3095"
        },
        {
          "reason": "Machine translation models require parallel text to learn mappings between languages during training.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1348",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:593"
        },
        {
          "reason": "Backpropagation depends on partial derivatives to compute gradients with respect to individual parameters in a network.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1365",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4105"
        },
        {
          "reason": "Recurrent neural networks are the foundational concept underlying RNN software implementations, which rely on their structure and behavior.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1725",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1838"
        },
        {
          "reason": "Seq2seq models rely on recurrent neural networks to process sequential input and generate sequential output.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1725",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:2124"
        },
        {
          "reason": "The attention mechanism is a core component of the Transformer architecture, and understanding Transformers is essential to grasping how attention functions in practice.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:2710",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3986"
        },
        {
          "reason": "The paper 'Attention Is All You Need' introduces the Transformer architecture, making knowledge of the Transformer essential to understanding the paper's core contributions.",
          "strength": 0.95,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:2710",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3987"
        },
        {
          "reason": "The Transformer architecture is a foundational model for modern machine translation systems.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:2710",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:593"
        },
        {
          "reason": "Convolutional neural networks are a specialized type of artificial neural network, requiring understanding of basic neural network architecture and function.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3095",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4771"
        },
        {
          "reason": "Convolutional neural networks are a specialized type of neural network, requiring understanding of basic neural network architecture and function.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3917",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4771"
        },
        {
          "reason": "The Transformer architecture is built entirely around the attention mechanism, which is essential for understanding its operation and design.",
          "strength": 0.95,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3986",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:2710"
        },
        {
          "reason": "The paper 'Attention Is All You Need' builds directly on the concept of attention mechanisms in machine learning, using them as the core architectural component.",
          "strength": 0.95,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3986",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3987"
        },
        {
          "reason": "The paper 'Attention Is All You Need' introduces the Transformer architecture, making it essential for understanding its design and functionality.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3987",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:2710"
        },
        {
          "reason": "Recurrent neural networks require backpropagation through time, which depends on backpropagation for gradient computation.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4105",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1725"
        },
        {
          "reason": "Seq2seq models use backpropagation to update parameters during training, making it essential for learning.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4105",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:2124"
        },
        {
          "reason": "Transformers rely on backpropagation to update parameters during training through gradient descent.",
          "strength": 0.95,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4105",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:2710"
        },
        {
          "reason": "Convolutional neural networks use backpropagation to adjust weights during training, making it essential for learning.",
          "strength": 0.95,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4105",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4771"
        },
        {
          "reason": "Backpropagation through time is essential for training recurrent neural networks, as it enables gradient computation across sequential time steps.",
          "strength": 0.95,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4110",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:1725"
        },
        {
          "reason": "Backpropagation relies on the chain rule to compute gradients through composed functions.",
          "strength": 0.95,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4419",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4105"
        },
        {
          "reason": "Convolutional neural networks rely on convolution operations to extract spatial features from input data.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4770",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4771"
        },
        {
          "reason": "Backpropagation requires the ability to compute gradients, which depends on functions being differentiable.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:5134",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4105"
        },
        {
          "reason": "Backpropagation depends on gradients to adjust model parameters during training.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:5908",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:4105"
        },
        {
          "reason": "A multilayer perceptron is a fundamental type of neural network, and understanding its structure is essential to grasping the broader concept of neural networks in machine learning.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:959",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3917"
        }
      ],
      "ABOUT": [
        {
          "reason": "The paper explicitly identifies self-attention as a fundamental concept behind the success of Transformers. It positions self-attention as a core component introduced in the foundational design of the model.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:17634",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3986"
        },
        {
          "reason": "The concept of attention is presented as a key architectural innovation and is classified as one of the seven main categories of CNN advancements. It is highlighted as a significant and popular approach in recent deep CNN designs.",
          "strength": 0.8,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:17806",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3986"
        },
        {
          "reason": "The attention mechanism is presented as one of several key innovations in the proposed method, central to its design and performance. It is introduced as a core component of the framework, contributing significantly to the model's effectiveness.",
          "strength": 0.8,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18015",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3986"
        },
        {
          "reason": "The paper presents the Transformer architecture with attention mechanisms as a core innovation, emphasizing the replacement of recurrence and convolutions with pure attention. The attention mechanism is central to the model's design and performance, though not explicitly named as a new concept beyond the overall architecture.",
          "strength": 0.8,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18188",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:11362"
        },
        {
          "reason": "The paper presents the Transformer as a model that is more parallelizable than previous architectures, highlighting reduced training time as a key advantage. This property is emphasized as a significant benefit of the proposed architecture.",
          "strength": 0.8,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18188",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:14531"
        },
        {
          "reason": "The paper explicitly claims to establish a new single-model state-of-the-art BLEU score on the WMT 2014 English-to-French task, positioning it as a key contribution.",
          "strength": 0.9,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18188",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:15883"
        },
        {
          "reason": "The paper explicitly introduces the Transformer architecture as a new, simple network design based solely on attention mechanisms. It positions the architecture as a key contribution, replacing recurrent and convolutional components entirely.",
          "strength": 0.95,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18188",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:2710"
        },
        {
          "reason": "The paper introduces the self-attention mechanism as a core component of the Transformer architecture, presenting it as a fundamental innovation that replaces recurrence and convolution. It is explicitly positioned as the key mechanism enabling the model's performance and parallelization.",
          "strength": 0.95,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18188",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:3986"
        },
        {
          "reason": "The paper introduces the Transformer architecture as a novel model based solely on attention mechanisms, explicitly dispensing with recurrence and convolutions. This establishes the attention-only mechanism as a core, proposed contribution.",
          "strength": 0.95,
          "source": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18188",
          "target": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:9544"
        }
      ]
    },
    "target_paper": {
      "citationCount": 162388,
      "name": "Attention is All you Need",
      "description": "Transformer은 순환 및 컨볼루션 구조를 제거하고 순수한 어텐션 메커니즘만을 사용하여 영어-독어 번역과 영어-프랑스어 번역에서 우수한 성능을 달성한 새로운 신경망 아키텍처이다.",
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
      "id": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:18188"
    }
  }
}