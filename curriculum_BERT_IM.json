{
  "graph_meta": {
    "paper_id": "26c69973-02be-4052-a794-6973546e8baf",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "summarize": "BERT 논문은 Transformer 기반의 양방향 언어 표현 모델을 제안한다. 기존 단방향 언어 모델의 한계를 극복하기 위해 마스크된 언어 모델(MLM)과 다음 문장 예측(NSP) 과제를 결합한 사전 훈련 방식을 도입한다. MLM은 입력 토큰의 일부를 무작위로 마스킹하고 주변 문맥을 활용해 이를 예측하는 방식으로 양방향 표현을 학습한다. NSP는 문장 쌍 간의 관계를 이해하는 능력을 향상시킨다. BERT는 사전 훈련 후 간단한 출력 계층 추가로 다양한 NLP 작업(질문 답변, 언어 추론 등)에 미세 조정될 수 있으며, 11개의 주요 NLP 작업에서 기존 방법을 크게 능가하는 성능을 달성했다. GLUE, SQuAD, SWAG 벤치마크에서 새로운 성능 기록을 수립했다."
  },
  "first_node_order": [
    "key-002",
    "key-011",
    "key-008",
    "key-009",
    "key-001",
    "key-003",
    "key-005",
    "key-007"
  ],
  "nodes": [
    {
      "keyword_id": "key-001",
      "keyword": "Fine-Tuning",
      "description": "Fine-Tuning은 BERT의 핵심 활용 방식으로, 사전 훈련된 모델을 특정 NLP 작업에 맞춰 미세 조정하는 과정을 이해해야 합니다. 논문의 'Fine-Tuning' 섹션에서 설명되며, GLUE/SQuAD 등 다양한 작업 적용 방식을 파악하는 데 필수적입니다. Attention과 Transformer 학습 후 이해해야 하는 후속 개념입니다.",
      "keyword_importance": 8,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-001",
          "resource_name": "BERT fine-tuning techniques : r/LanguageTechnology",
          "url": "https://www.reddit.com/r/LanguageTechnology/comments/pwj8bi/bert_finetuning_techniques/",
          "type": "web_doc",
          "resource_description": "BERT의 일부 레이어만 미세 조정하는 기법을 설명하여, 중간 수준 학습자가 효율적으로 파인튜닝을 적용하는 데 도움을 주는 자료입니다.",
          "difficulty": 4,
          "importance": 8,
          "study_load": 0.5,
          "is_necessary": true
        },
        {
          "resource_id": "res-010",
          "resource_name": "How to share BERT between tasks in multi-task setting? #504 - GitHub",
          "url": "https://github.com/google-research/bert/issues/504",
          "type": "web_doc",
          "resource_description": "BERT 모델을 다중 작업 환경에서 공유하여 미세 조정하는 방법을 설명하는 자료로, GLUE 데이터셋 기반 다중 작업 학습 시 Fine-Tuning 전략 이해에 직접적으로 도움됩니다.",
          "difficulty": 6,
          "importance": 8,
          "study_load": 1.5,
          "is_necessary": false
        }
      ],
      "resource_reason": "제공된 자료는 BERT의 일부 레이어만 미세 조정하는 특정 기법에 초점을 맞추고 있어, Fine-Tuning의 전반적인 개념, GLUE/SQuAD 적용 사례, Attention/Transformer와의 연계성 등 핵심 요소를 충분히 다루지 못합니다."
    },
    {
      "keyword_id": "key-002",
      "keyword": "Attention",
      "description": "Attention은 Transformer의 핵심 메커니즘으로, BERT의 양방향 문맥 이해 기반입니다. 'Attention Is All You Need' 논문 개념을 선행 학습해야 BERT의 레이어 동작을 이해할 수 있습니다. Transformer 구조 학습의 필수 선수 지식입니다.",
      "keyword_importance": 9,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-002",
          "resource_name": "BERT (language model)",
          "url": "https://en.wikipedia.org/wiki/BERT_(language_model)",
          "type": "web_doc",
          "resource_description": "BERT 언어 모델과 Attention의 직접적인 연관성이 낮아 필수 학습 자료는 아니지만, Google의 다양한 소프트웨어 개발 도구 목록을 통해 간접적으로 관련 기술 스택을 이해하는 데 도움이 됩니다.",
          "difficulty": 8,
          "importance": 3,
          "study_load": 2.5,
          "is_necessary": false
        },
        {
          "resource_id": "res-011",
          "resource_name": "How BERT leverage attention mechanism and transformer to learn ...",
          "url": "https://medium.com/data-science/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb",
          "type": "web_doc",
          "resource_description": "BERT의 양방향 트랜스포머와 어텐션 메커니즘 활용 방식을 설명하여 intermediate 학습자가 어텐션의 핵심 개념을 이해하는 데 도움을 주는 자료입니다.",
          "difficulty": 5,
          "importance": 8,
          "study_load": 1.5,
          "is_necessary": true
        }
      ],
      "resource_reason": "제공된 자료는 BERT 언어 모델에 대한 일반적인 설명만 포함하고 있으며, Attention 메커니즘의 핵심 개념과 'Attention Is All You Need' 논문의 내용을 다루지 않아 키워드 이해에 불충분합니다."
    },
    {
      "keyword_id": "key-003",
      "keyword": "Language Representation Model",
      "description": "Language Representation Model은 BERT의 목적을 이해하는 데 필요합니다. ELMo/GPT 등 기존 모델과의 비교를 통해 BERT의 혁신성(양방향성)을 파악할 수 있습니다. 논문의 서론 및 관련 연구 섹션과 직접 연결됩니다.",
      "keyword_importance": 7,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-003",
          "resource_name": "Application of the Bidirectional Encoder Representations from ...",
          "url": "https://formative.jmir.org/2025/1/e67311",
          "type": "paper",
          "resource_description": "BERT 모델의 성능을 다른 사전 학습 모델과 비교한 실험 결과를 제공하여, 양방향 인코더 표현 모델의 우수성을 이해하는 데 필수적인 자료입니다.",
          "difficulty": 7,
          "importance": 9,
          "study_load": 1.5,
          "is_necessary": true
        }
      ]
    },
    {
      "keyword_id": "key-004",
      "keyword": "Bert",
      "description": "BERT는 논문의 주제이지만, 사전 훈련/미세 조정 프로세스와 MLM/NSP 메커니즘을 체계적으로 학습해야 합니다. Transformer와 Masked Language Model 이해 후 접근해야 하는 최종 개념입니다.",
      "keyword_importance": 10,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-004",
          "resource_name": "BERT: Pre-training of Deep Bidirectional Transformers for Language ...",
          "url": "https://arxiv.org/abs/1810.04805",
          "type": "paper",
          "resource_description": "BERT 모델의 핵심 개념과 양방향 트랜스포머 기반 사전 학습 방식을 깊이 이해할 수 있어 NLP 분야 intermediate 학습자에게 필수적인 논문입니다.",
          "difficulty": 7,
          "importance": 9,
          "study_load": 3.5,
          "is_necessary": true
        },
        {
          "resource_id": "res-012",
          "resource_name": "[논문 리뷰] BERT: Pre-training of Deep Bidirectional Transformers for ...",
          "url": "https://velog.io/@dltpdl31/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding",
          "type": "paper",
          "resource_description": "BERT 모델의 핵심 개념과 학습 방식을 체계적으로 설명한 논문 리뷰로, intermediate 학습자가 NLP 최신 기술을 이해하는 데 필수적인 자료입니다.",
          "difficulty": 6,
          "importance": 9,
          "study_load": 2.5,
          "is_necessary": false
        }
      ],
      "resource_reason": "BERT의 MLM/NSP 메커니즘과 사전 훈련/미세 조정 프로세스를 체계적으로 학습하기 위해 추가 자료(예: Transformer 기초, MLM 설명 자료)가 필요하며, 현재 제공된 단일 논문만으로는 intermediate 학습자의 이해를 완전히 지원하기에 부족합니다."
    },
    {
      "keyword_id": "key-005",
      "keyword": "Textual Entailment",
      "description": "Textual Entailment는 BERT가 해결한 주요 NLP 작업 중 하나로, MultiNLI 데이터셋 성능 평가 시 필요합니다. NLP Tasks의 하위 개념으로, BERT의 실용적 적용 사례를 이해하는 데 기여합니다.",
      "keyword_importance": 6,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-005",
          "resource_name": "[PDF] RESEARCH ARTICLE Comparing BERT against Traditional ...",
          "url": "https://repositorio.comillas.edu/xmlui/bitstream/handle/11531/78847/document.pdf?sequence=1",
          "type": "paper",
          "resource_description": "BERT와 전통적 NLP 접근법을 비교하는 연구 논문으로, Textual Entailment와 직접 관련은 낮지만 전이 학습 기반 모델의 성능 우수성을 이해하는 데 도움됨",
          "difficulty": 6,
          "importance": 7,
          "study_load": 2.5,
          "is_necessary": false
        },
        {
          "resource_id": "res-013",
          "resource_name": "dh1105/Sentence-Entailment - MultiNLI and SNLI. - GitHub",
          "url": "https://github.com/dh1105/Sentence-Entailment",
          "type": "web_doc",
          "resource_description": "BERT, ALBERT, BiLSTM 등 다양한 딥러닝 모델을 활용한 문장 함의 평가 벤치마킹을 통해 Textual Entailment의 실용적 적용 방법을 학습할 수 있어 intermediate 학습자에게 유용합니다.",
          "difficulty": 6,
          "importance": 8,
          "study_load": 3.5,
          "is_necessary": false
        }
      ],
      "resource_reason": "제공된 자료는 Textual Entailment와 직접적인 관련성이 낮으며, 해당 개념을 명확히 설명하기 위한 충분한 학습 자료가 부족합니다."
    },
    {
      "keyword_id": "key-006",
      "keyword": "Transformer",
      "description": "Transformer는 BERT의 기반 아키텍처로, Self-Attention과 멀티헤드 어텐션 구조를 이해해야 합니다. 논문의 'Architecture' 섹션에서 상세히 설명되며, BERT 구현의 필수 선행 지식입니다.",
      "keyword_importance": 9,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-006",
          "resource_name": "BERT (language model) - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/BERT_(language_model)",
          "type": "web_doc",
          "resource_description": "BERT와 Transformer의 직접적인 연관성이 낮아 중요도가 낮지만, Google의 소프트웨어 생태계 이해를 통해 간접적으로 BERT 개발 배경을 파악할 수 있습니다.",
          "difficulty": 3,
          "importance": 2,
          "study_load": 0.5,
          "is_necessary": true
        },
        {
          "resource_id": "res-014",
          "resource_name": "A Deep Dive into the Self-Attention Mechanism of Transformers",
          "url": "https://medium.com/analytics-vidhya/a-deep-dive-into-the-self-attention-mechanism-of-transformers-fe943c77e654",
          "type": "web_doc",
          "resource_description": "Self-Attention 메커니즘의 핵심 원리와 계산 과정을 상세히 설명하여 Transformer 구조 이해에 필수적인 내용을 체계적으로 학습할 수 있습니다.",
          "difficulty": 6,
          "importance": 9,
          "study_load": 2.5,
          "is_necessary": false
        }
      ],
      "resource_reason": "제공된 자료는 BERT의 개발 배경을 다루지만, Transformer의 핵심 개념인 Self-Attention과 멀티헤드 어텐션 구조에 대한 직접적인 설명이 부족합니다."
    },
    {
      "keyword_id": "key-007",
      "keyword": "Nlp Tasks",
      "description": "NLP Tasks는 BERT의 성능 검증 대상(질문 응답, 언어 추론 등)을 이해하는 데 필요합니다. GLUE/SQuAD 등 벤치마크와의 연관성을 파악하려면 작업별 특성을 선행 학습해야 합니다.",
      "keyword_importance": 7,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-007",
          "resource_name": "Comparison of BERT implementations for natural language ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S2352914822002763",
          "type": "paper",
          "resource_description": "의료 분야 NLP 작업에 특화된 BERT 모델들의 성능 비교를 다루어, 중간 수준 학습자가 도메인 적응형 모델 선택 시 참고할 수 있는 실용적인 인사이트를 제공합니다.",
          "difficulty": 6,
          "importance": 8,
          "study_load": 2.5,
          "is_necessary": false
        },
        {
          "resource_id": "res-015",
          "resource_name": "Pruning BERT on a GLUE task - Wandb",
          "url": "https://wandb.ai/katia/GLUE-aws-sweep/reports/Pruning-BERT-on-a-GLUE-task--Vmlldzo4NTYxNjI",
          "type": "web_doc",
          "resource_description": "GLUE 벤치마크와 BERT 모델 경량화 방법을 다루어 NLP 태스크 이해에 직접적으로 기여하므로, 중급 학습자에게 유용한 자료입니다.",
          "difficulty": 6,
          "importance": 8,
          "study_load": 2.5,
          "is_necessary": false
        }
      ],
      "resource_reason": "제공된 자료는 의료 분야 NLP 작업에 특화된 BERT 모델 비교에 집중되어 있어, 일반적인 NLP Tasks(질문 응답, 언어 추론 등)와 GLUE/SQuAD 벤치마크의 연관성을 포괄적으로 학습하기에는 부족합니다."
    },
    {
      "keyword_id": "key-008",
      "keyword": "Masked Language Model",
      "description": "Masked Language Model은 BERT의 사전 훈련 핵심 기법으로, 양방향 문맥 학습 메커니즘을 이해해야 합니다. 논문의 'Pre-Training' 섹션에서 상세히 다뤄지며, NSP와 함께 BERT의 독창성을 설명하는 요소입니다.",
      "keyword_importance": 8,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-008",
          "resource_name": "BERT : Architecture & Algorithm Explained | StudySmarter",
          "url": "https://www.studysmarter.co.uk/explanations/engineering/artificial-intelligence-engineering/bert/",
          "type": "web_doc",
          "resource_description": "BERT의 핵심 학습 방식인 Masked Language Model(MLM)을 중간 수준의 학습자가 이해하기 쉽게 설명한 자료로, 문맥 이해 능력 향상에 필수적인 개념을 다룹니다.",
          "difficulty": 4,
          "importance": 8,
          "study_load": 1.5,
          "is_necessary": true
        }
      ]
    },
    {
      "keyword_id": "key-009",
      "keyword": "Next Sentence Prediction",
      "description": "Next Sentence Prediction은 문장 간 관계 학습을 위한 BERT의 사전 훈련 과제입니다. 논문의 실험 결과에서 NSP 제거 시 성능 저하 현상을 분석할 때 필요하며, 언어 추론 작업과의 연관성이 높습니다.",
      "keyword_importance": 7,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-009",
          "resource_name": "BERT Transformers for Natural Language Processing",
          "url": "https://blog.paperspace.com/bert-natural-language-processing/",
          "type": "web_doc",
          "resource_description": "BERT의 사전 학습 과제 중 하나인 Next Sentence Prediction(NSP)을 이해하는 데 필수적인 내용을 간결하게 설명하여, 중급 학습자가 핵심 개념을 빠르게 습득하는 데 유용합니다.",
          "difficulty": 4,
          "importance": 8,
          "study_load": 1.0,
          "is_necessary": true
        }
      ]
    },
    {
      "keyword_id": "key-010",
      "keyword": "Natural Language Inference (NLI)",
      "description": "Natural Language Inference (NLI)는 BERT가 성능을 크게 향상시킨 주요 NLP 작업 중 하나입니다. 논문의 실험 섹션에서 MultiNLI 데이터셋 결과를 분석할 때 NLI의 개념(문장 간 함의 관계 분류)을 이해해야 합니다. 또한 NSP 과제와 NLI의 연관성을 파악하는 데 필수적입니다. BERT의 언어 추론 능력을 평가하는 핵심 지표이므로, 해당 작업 배경을 먼저 학습해야 합니다.",
      "keyword_importance": 7,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-016",
          "resource_name": "BERT and Transfer Learning in NLP",
          "url": "https://medium.com/@mervebdurna/bert-and-transfer-learning-in-nlp-11fc19435fa0",
          "type": "web_doc",
          "resource_description": "ALBERT의 경량화 기법을 통해 BERT 기반 NLI 모델 최적화에 도움을 주지만, NLI 직접 설명은 부족해 중요도가 다소 낮습니다.",
          "difficulty": 5,
          "importance": 7,
          "study_load": 1.5,
          "is_necessary": false
        },
        {
          "resource_id": "res-019",
          "resource_name": "Natural Language Inference (NLI) - Emergent Mind",
          "url": "https://www.emergentmind.com/topics/natural-language-inference-nli",
          "type": "web_doc",
          "resource_description": "NLI의 기본 개념과 평가 방법을 간결하게 설명하여 중급 학습자가 핵심 내용을 빠르게 이해하는 데 유용합니다.",
          "difficulty": 4,
          "importance": 8,
          "study_load": 1.0,
          "is_necessary": false
        }
      ],
      "resource_reason": "제공된 자료는 NLI를 직접 설명하지 않고 BERT 기반 NLI 모델 최적화에 초점을 맞추어 핵심 개념 학습에 불충분합니다."
    },
    {
      "keyword_id": "key-011",
      "keyword": "Bidirectional Encoder Representations",
      "description": "Bidirectional Encoder Representations는 BERT의 핵심 개념으로, 모델 이름(BERT: Bidirectional Encoder Representations from Transformers)에 직접 반영되었습니다. 논문의 서론과 방법론 섹션에서 단방향 모델과의 차이점을 이해하려면 양방향 문맥 학습의 필요성을 먼저 파악해야 합니다. MLM 과제와 결합해 모든 레이어에서 양방향 표현을 학습하는 메커니즘을 설명하는 기반 지식입니다.",
      "keyword_importance": 9,
      "is_keyword_necessary": true,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-017",
          "resource_name": "Bidirectional Transformer Encoders - Emergent Mind",
          "url": "https://www.emergentmind.com/topics/bidirectional-transformer-encoder",
          "type": "web_doc",
          "resource_description": "Bidirectional Transformer 인코더의 핵심 개념과 작동 방식을 명확히 설명하여, BERT와 같은 양방향 모델 이해에 필수적인 기초 지식을 제공합니다.",
          "difficulty": 6,
          "importance": 8,
          "study_load": 1.5,
          "is_necessary": true
        }
      ]
    },
    {
      "keyword_id": "key-012",
      "keyword": "Transformer Encoder",
      "description": "Transformer Encoder는 BERT의 기본 아키텍처로, 논문에서 제시된 모든 실험 결과의 기반이 됩니다. Transformer 구조(key-006)를 학습한 후 인코더 스택의 작동 방식을 구체적으로 이해해야 BERT의 구현 세부사항(예: 멀티헤드 어텐션, 위치 임베딩)을 파악할 수 있습니다. 논문의 '모델 아키텍처' 섹션과 직접적으로 연결되는 필수 선수 지식입니다.",
      "keyword_importance": 8,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-018",
          "resource_name": "How is BERT different from the original transformer architecture?",
          "url": "https://ai.stackexchange.com/questions/23221/how-is-bert-different-from-the-original-transformer-architecture",
          "type": "web_doc",
          "resource_description": "BERT와 원본 Transformer 아키텍처의 차이점을 명확히 설명하여 Transformer Encoder의 핵심 개념을 이해하는 데 도움을 주는 자료입니다.",
          "difficulty": 4,
          "importance": 8,
          "study_load": 1.0,
          "is_necessary": false
        },
        {
          "resource_id": "res-020",
          "resource_name": "Tutorial 6: Transformers and Multi-Head Attention",
          "url": "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html",
          "type": "web_doc",
          "resource_description": "Transformer Encoder의 핵심 구성 요소인 Multi-Head Attention과 구현 방식을 코드 예제와 함께 설명하여 intermediate 학습자가 실제 구현에 필요한 개념을 체계적으로 이해할 수 있도록 돕는 자료입니다.",
          "difficulty": 6,
          "importance": 8,
          "study_load": 2.5,
          "is_necessary": false
        }
      ],
      "resource_reason": "제공된 자료는 BERT와 원본 Transformer의 차이점에 초점을 맞추고 있지만, Transformer Encoder의 구체적인 작동 방식(예: 멀티헤드 어텐션, 위치 임베딩 구현)을 설명하는 핵심 내용이 부족합니다."
    },
    {
      "keyword_id": "key-013",
      "keyword": "Masked Language Model (MLM)",
      "description": "Masked Language Model (MLM)은 BERT의 핵심 사전 훈련 방식으로, 입력 토큰의 일부를 마스킹하고 주변 문맥을 활용해 예측하는 과정을 통해 양방향 언어 표현을 학습합니다. 논문의 3.1절과 4.1절에서 MLM의 구현 및 실험 결과가 상세히 설명되며, BERT의 성능 향상 메커니즘을 이해하는 데 필수적입니다. 기존 단방향 모델과의 차별성을 파악하려면 MLM을 먼저 이해해야 합니다.",
      "keyword_importance": 9,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-021",
          "resource_name": "Cloze Task for Training in NLP - Emergent Mind",
          "url": "https://www.emergentmind.com/topics/cloze-task-for-training",
          "type": "web_doc",
          "resource_description": "이 자료는 MLM의 핵심 학습 방법인 Cloze task를 다루며, 중간 수준 학습자가 MLM의 작동 원리와 응용 분야(예: 수학적 추론)를 이해하는 데 유용합니다.",
          "difficulty": 5,
          "importance": 8,
          "study_load": 1.5,
          "is_necessary": false
        }
      ]
    },
    {
      "keyword_id": "key-014",
      "keyword": "Next Sentence Prediction (NSP)",
      "description": "Next Sentence Prediction (NSP)은 문장 간 관계 이해를 위한 BERT의 사전 훈련 과제로, 논문의 3.2절과 4.2절에서 설명됩니다. NSP는 언어 추론(NLI) 및 질문 답변과 같은 다운스트림 작업 성능에 직접적인 영향을 미치며, BERT의 문장 수준 표현 학습 방식을 파악하는 데 중요합니다. 다만 MLM에 비해 상대적 중요도는 낮습니다.",
      "keyword_importance": 7,
      "is_keyword_necessary": false,
      "is_resource_sufficient": true,
      "resources": [
        {
          "resource_id": "res-022",
          "resource_name": "BERT - Vishal Ramesh",
          "url": "https://bbloggsbott.github.io/read-a-paper/bert/",
          "type": "web_doc",
          "resource_description": "BERT의 핵심 메커니즘인 NSP와 양방향성(MLM)의 중요성을 실험 결과와 함께 설명하여, 중급 학습자가 트랜스포머 기반 모델의 전이 학습 방식을 이해하는 데 유용합니다.",
          "difficulty": 5,
          "importance": 8,
          "study_load": 1.5,
          "is_necessary": false
        }
      ]
    }
  ],
  "edges": [
    {
      "start": "key-002",
      "end": "key-006"
    },
    {
      "start": "key-006",
      "end": "key-004"
    },
    {
      "start": "key-001",
      "end": "26c69973-02be-4052-a794-6973546e8baf"
    },
    {
      "start": "key-003",
      "end": "26c69973-02be-4052-a794-6973546e8baf"
    },
    {
      "start": "key-007",
      "end": "26c69973-02be-4052-a794-6973546e8baf"
    },
    {
      "start": "key-005",
      "end": "26c69973-02be-4052-a794-6973546e8baf"
    },
    {
      "start": "key-004",
      "end": "26c69973-02be-4052-a794-6973546e8baf"
    },
    {
      "start": "4:b6e3ffac-75a6-4c31-92d7-0b650c400d49:6655",
      "end": "26c69973-02be-4052-a794-6973546e8baf"
    },
    {
      "start": "key-006",
      "end": "26c69973-02be-4052-a794-6973546e8baf"
    },
    {
      "start": "key-008",
      "end": "26c69973-02be-4052-a794-6973546e8baf"
    },
    {
      "start": "key-009",
      "end": "26c69973-02be-4052-a794-6973546e8baf"
    },
    {
      "start": "key-005",
      "end": "key-010"
    },
    {
      "start": "key-012",
      "end": "key-004"
    },
    {
      "start": "key-006",
      "end": "key-012"
    },
    {
      "start": "key-011",
      "end": "key-004"
    },
    {
      "start": "key-011",
      "end": "26c69973-02be-4052-a794-6973546e8baf"
    },
    {
      "start": "key-013",
      "end": "26c69973-02be-4052-a794-6973546e8baf"
    },
    {
      "start": "key-014",
      "end": "key-010"
    },
    {
      "start": "key-014",
      "end": "26c69973-02be-4052-a794-6973546e8baf"
    },
    {
      "start": "key-004",
      "end": "key-013"
    },
    {
      "start": "key-004",
      "end": "key-014"
    }
  ]
}